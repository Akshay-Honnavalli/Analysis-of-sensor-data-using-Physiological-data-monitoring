{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7880750,"sourceType":"datasetVersion","datasetId":4625365},{"sourceId":8009007,"sourceType":"datasetVersion","datasetId":4717292},{"sourceId":8860524,"sourceType":"datasetVersion","datasetId":5334482},{"sourceId":8860538,"sourceType":"datasetVersion","datasetId":5334491},{"sourceId":9381856,"sourceType":"datasetVersion","datasetId":5691656},{"sourceId":9417760,"sourceType":"datasetVersion","datasetId":5719762},{"sourceId":9419954,"sourceType":"datasetVersion","datasetId":5721394},{"sourceId":9585928,"sourceType":"datasetVersion","datasetId":5845787},{"sourceId":9586037,"sourceType":"datasetVersion","datasetId":5845862},{"sourceId":9593785,"sourceType":"datasetVersion","datasetId":5851860},{"sourceId":9606542,"sourceType":"datasetVersion","datasetId":5861277},{"sourceId":9606618,"sourceType":"datasetVersion","datasetId":5861337},{"sourceId":9911682,"sourceType":"datasetVersion","datasetId":6090166}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-16T02:26:29.587084Z","iopub.execute_input":"2024-11-16T02:26:29.58745Z","iopub.status.idle":"2024-11-16T02:26:30.055622Z","shell.execute_reply.started":"2024-11-16T02:26:29.587415Z","shell.execute_reply":"2024-11-16T02:26:30.054725Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Read csv","metadata":{}},{"cell_type":"code","source":"# df=pd.read_csv('/kaggle/input/subject3/10003_26038.csv')\n# df=pd.read_csv('/kaggle/input/subject-5/10015_10063.csv')\n# df=pd.read_csv('/kaggle/input/subject-9/12808_17752.csv')\n# df=pd.read_csv('/kaggle/input/testannotation/10000_17728.csv')\ndf=pd.read_csv('/kaggle/input/9955-18184/9955_18184.csv')\n# df=pd.read_csv('/kaggle/input/9931-17578/9931_17578.csv')\n# df=pd.read_csv('/kaggle/input/subject-5/10015_10063.csv')\n\n\nprint(df.isna().sum())\ndf_og = df","metadata":{"execution":{"iopub.status.busy":"2024-11-16T02:26:33.630456Z","iopub.execute_input":"2024-11-16T02:26:33.630982Z","iopub.status.idle":"2024-11-16T02:28:15.720934Z","shell.execute_reply.started":"2024-11-16T02:26:33.630946Z","shell.execute_reply":"2024-11-16T02:28:15.719845Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n = len(df)\nn","metadata":{"execution":{"iopub.status.busy":"2024-11-16T02:28:32.29019Z","iopub.execute_input":"2024-11-16T02:28:32.290915Z","iopub.status.idle":"2024-11-16T02:28:32.297413Z","shell.execute_reply.started":"2024-11-16T02:28:32.290881Z","shell.execute_reply":"2024-11-16T02:28:32.296593Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## remove rows with \"sleep stage ?\"","metadata":{}},{"cell_type":"code","source":"df = df_og","metadata":{"execution":{"iopub.status.busy":"2024-10-12T07:19:05.998Z","iopub.execute_input":"2024-10-12T07:19:05.998343Z","iopub.status.idle":"2024-10-12T07:19:06.002393Z","shell.execute_reply.started":"2024-10-12T07:19:05.998314Z","shell.execute_reply":"2024-10-12T07:19:06.001608Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# n = len(df)\ndf = df.iloc[0:8033528]\nlen(df)","metadata":{"execution":{"iopub.status.busy":"2024-10-28T10:21:32.835266Z","iopub.execute_input":"2024-10-28T10:21:32.836072Z","iopub.status.idle":"2024-10-28T10:21:32.84207Z","shell.execute_reply.started":"2024-10-28T10:21:32.836038Z","shell.execute_reply":"2024-10-28T10:21:32.841142Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2024-11-01T05:13:43.600736Z","iopub.execute_input":"2024-11-01T05:13:43.601663Z","iopub.status.idle":"2024-11-01T05:13:43.608902Z","shell.execute_reply.started":"2024-11-01T05:13:43.601623Z","shell.execute_reply":"2024-11-01T05:13:43.60789Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df.isna().sum())","metadata":{"execution":{"iopub.status.busy":"2024-11-01T05:13:44.414927Z","iopub.execute_input":"2024-11-01T05:13:44.415821Z","iopub.status.idle":"2024-11-01T05:13:44.701517Z","shell.execute_reply.started":"2024-11-01T05:13:44.415778Z","shell.execute_reply":"2024-11-01T05:13:44.700503Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n\n# Print maximum values of all columns\nprint(\"Maximum values of all columns:\")\nprint(df.max())\n\n# Print minimum values of all columns\nprint(\"\\nMinimum values of all columns:\")\nprint(df.min())\n","metadata":{"execution":{"iopub.status.busy":"2024-11-02T05:28:21.358926Z","iopub.execute_input":"2024-11-02T05:28:21.359725Z","iopub.status.idle":"2024-11-02T05:28:22.515238Z","shell.execute_reply.started":"2024-11-02T05:28:21.359688Z","shell.execute_reply":"2024-11-02T05:28:22.51416Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from scipy.signal import savgol_filter\nprint(df.columns)\n# Define the common window size\ncommon_window_size = 512  # Approximately 2 seconds of data\n\n# Apply the Moving Average filter to each column\nsmoothed_df = df.rolling(window=common_window_size, min_periods=1).mean()\n\n# smoothed_ema = df.ewm(span=common_window_size, adjust=False).mean()\n\n# # Define the smoothing parameters\n# window_size = 513  # Must be odd and chosen based on your data's characteristics\n# poly_order = 2   # Polynomial order for Savitzky-Golay filter\n\n# # Apply Savitzky-Golay Filter to each column\n# smoothed_sg = df.apply(lambda x: savgol_filter(x, window_size, poly_order))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T06:48:15.295748Z","iopub.execute_input":"2024-10-16T06:48:15.296568Z","iopub.status.idle":"2024-10-16T06:48:17.18369Z","shell.execute_reply.started":"2024-10-16T06:48:15.296537Z","shell.execute_reply":"2024-10-16T06:48:17.181902Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df=df.drop(columns=['Patient Event'])\nfrom sklearn.preprocessing import StandardScaler\n\n# Select the columns to be standardized\ncolumns_to_standardize = ['Patient Event', 'EOG LOC-M2', 'EOG ROC-M1', 'EMG Chin1-Chin2',\n       'EEG F3-M2', 'EEG F4-M1', 'EEG C3-M2', 'EEG C4-M1', 'EEG O1-M2',\n       'EEG O2-M1', 'EEG CZ-O1', 'EMG LLeg-RLeg', 'ECG EKG2-EKG', 'Snore',\n       'Resp PTAF', 'Resp Airflow', 'Resp Thoracic', 'Resp Abdominal', 'SpO2',\n       'Rate', 'EtCO2', 'Capno', 'Resp Rate', 'C-flow', 'Tidal Vol',\n       'Pressure']\n\n# Initialize the StandardScaler\nscaler = StandardScaler()\n\n\n# Standardize the selected columns\nsmoothed_df[columns_to_standardize] = scaler.fit_transform(smoothed_df[columns_to_standardize])","metadata":{"execution":{"iopub.status.busy":"2024-09-18T11:24:35.257548Z","iopub.execute_input":"2024-09-18T11:24:35.258248Z","iopub.status.idle":"2024-09-18T11:24:39.549171Z","shell.execute_reply.started":"2024-09-18T11:24:35.258216Z","shell.execute_reply":"2024-09-18T11:24:39.548375Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(smoothed_df)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T11:24:42.895928Z","iopub.execute_input":"2024-09-18T11:24:42.896743Z","iopub.status.idle":"2024-09-18T11:24:45.510381Z","shell.execute_reply.started":"2024-09-18T11:24:42.896704Z","shell.execute_reply":"2024-09-18T11:24:45.509388Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Standardizing columns (MinMax scaling)","metadata":{}},{"cell_type":"code","source":"w_df.columns","metadata":{"execution":{"iopub.status.busy":"2024-09-21T07:05:47.408632Z","iopub.execute_input":"2024-09-21T07:05:47.409205Z","iopub.status.idle":"2024-09-21T07:05:47.414838Z","shell.execute_reply.started":"2024-09-21T07:05:47.409179Z","shell.execute_reply":"2024-09-21T07:05:47.413911Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\n\n# Select the columns to be standardized\ncolumns_to_scale = 'EOG LOC-M2', 'EOG ROC-M1', 'EEG Chin1-Chin2', 'EEG F3-M2', 'EEG F4-M1',\n       'EEG C3-M2', 'EEG C4-M1', 'EEG O1-M2', 'EEG O2-M1', 'EEG CZ-O1',\n       'EMG LLeg-RLeg', 'ECG EKG2-EKG', 'Snore', 'Resp PTAF', 'Resp Airflow',\n       'Resp Thoracic', 'Resp Abdominal', 'SpO2', 'Rate', 'Capno', 'Resp Rate',\n       'C-flow', 'Tidal Vol', 'Pressure']\n# 'EtCO2'\n\n# Initialize the StandardScaler\n# scaler = MinMaxScaler(feature_range=(0, 10))\nscaler = RobustScaler()\nrobust_scaled_df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n\n\n# Scale the selected columns\n# smoothed_df[columns_to_scale] = scaler.fit_transform(smoothed_df[columns_to_scale])\nw_df[columns_to_scale] = scaler.fit_transform(w_df[columns_to_scale])\n\n# Print the result to verify\nprint(r_df[columns_to_scale].head())\n","metadata":{"execution":{"iopub.status.busy":"2024-09-21T07:05:47.319085Z","iopub.status.idle":"2024-09-21T07:05:47.319402Z","shell.execute_reply.started":"2024-09-21T07:05:47.31924Z","shell.execute_reply":"2024-09-21T07:05:47.319252Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# smoothed_df=smoothed_df.drop(columns=['Patient Event'])","metadata":{"execution":{"iopub.status.busy":"2024-09-17T06:07:24.541017Z","iopub.execute_input":"2024-09-17T06:07:24.541637Z","iopub.status.idle":"2024-09-17T06:07:25.297964Z","shell.execute_reply.started":"2024-09-17T06:07:24.541606Z","shell.execute_reply":"2024-09-17T06:07:25.296739Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# data_converted=df['EEG F4-M1']\nsmoothed_df.shape\nsmoothed_df.head()\n# smoothed_df=smoothed_df.drop(columns=['Patient Event'])","metadata":{"execution":{"iopub.status.busy":"2024-09-18T11:24:52.669065Z","iopub.execute_input":"2024-09-18T11:24:52.669684Z","iopub.status.idle":"2024-09-18T11:24:52.700227Z","shell.execute_reply.started":"2024-09-18T11:24:52.669653Z","shell.execute_reply":"2024-09-18T11:24:52.699299Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"smoothed_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-18T04:18:04.61568Z","iopub.execute_input":"2024-09-18T04:18:04.616048Z","iopub.status.idle":"2024-09-18T04:18:04.623064Z","shell.execute_reply.started":"2024-09-18T04:18:04.616019Z","shell.execute_reply":"2024-09-18T04:18:04.621906Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA (anomaly list)","metadata":{}},{"cell_type":"code","source":"# df1=pd.read_csv('/kaggle/input/subject3/10003_26038 (1).tsv',sep='\\t')\n# df1=pd.read_csv('/kaggle/input/subject-5/10015_10063.tsv',sep='\\t')\n# df1=pd.read_csv('/kaggle/input/subject-9/12808_17752.tsv',sep='\\t')\n# df1=pd.read_csv('/kaggle/input/testfinal/10000_17728 (1).tsv',sep='\\t')\ndf1=pd.read_csv('/kaggle/input/9955-18184-tsv/9955_18184.tsv',sep='\\t')\n# df1=pd.read_csv('/kaggle/input/9931-17578-tsv/9931_17578.tsv',sep='\\t')\n# df1=pd.read_csv('/kaggle/input/subject-5/10015_10063.tsv',sep='\\t')\n\n\n\nprint(set(df1['description'].values))","metadata":{"execution":{"iopub.status.busy":"2024-11-16T02:28:50.95684Z","iopub.execute_input":"2024-11-16T02:28:50.957443Z","iopub.status.idle":"2024-11-16T02:28:50.979027Z","shell.execute_reply.started":"2024-11-16T02:28:50.957412Z","shell.execute_reply":"2024-11-16T02:28:50.97811Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# anomalies_list = [\n#     'Central Apnea', 'Grit Teeth or Chew (5 seconds)', 'Breath Hold (10 seconds)',\n#     'Simulate Snore or Hum (5 seconds)','Obstructive Apnea','Obstructive Hypopnea',\n#     'Looking around', 'Eyes Up and Down X5',\n#     'Mixed Apnea', 'Oral Breathing (10 seconds)', 'Oxygen Desaturation',\n#     'Flex Right Foot', 'So', 'Flex Left Foot', 'Eye Blinks X5', 'Bruxism', 'Eyes Left and Right X5'\n# ]\nanomalies_list = ['Oxygen Desaturation']\n# anomalies_list = ['Obstructive Apnea','Mixed Apnea','Central Apnea']\n\n# Assume df1 is your DataFrame containing the annotations\n# Filter annotations dataframe to include only anomalies\nanomalies_df = df1[df1['description'].isin(anomalies_list)]\n\n# Initialize the binary data list with zeros\n# binary_data1 = [0] * len(smoothed_df)\nbinary_data1 = [0] * len(df)\n\n# Iterate over each row in anomalies_df\nfor _, row in anomalies_df.iterrows():\n    start_row = int(row['onset'] * 256)\n    end_row = int((row['onset'] + row['duration']) * 256)\n    # Mark intervals as 1\n    for i in range(start_row, end_row + 1):\n        if i < len(binary_data1):\n            binary_data1[i] = 1\n\n# Add the binary data as a new column to smoothed_df\n# smoothed_df['anomalies'] = binary_data1\ndf['anomalies'] = binary_data1\n\n# Display the updated DataFrame\nprint(df.head())","metadata":{"execution":{"iopub.status.busy":"2024-11-16T02:29:03.467026Z","iopub.execute_input":"2024-11-16T02:29:03.467371Z","iopub.status.idle":"2024-11-16T02:29:07.597152Z","shell.execute_reply.started":"2024-11-16T02:29:03.467344Z","shell.execute_reply":"2024-11-16T02:29:07.596128Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(binary_data1.count(1))","metadata":{"execution":{"iopub.status.busy":"2024-11-16T02:29:14.179901Z","iopub.execute_input":"2024-11-16T02:29:14.180279Z","iopub.status.idle":"2024-11-16T02:29:14.270162Z","shell.execute_reply.started":"2024-11-16T02:29:14.180247Z","shell.execute_reply":"2024-11-16T02:29:14.269196Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Percentage of W stage:\",((len(w_df)/(len(w_df)+len(n1_df)+len(n2_df)+len(n3_df)+len(r_df)))*100))\nprint(\"Percentage of N1 stage:\",((len(n1_df)/(len(w_df)+len(n1_df)+len(n2_df)+len(n3_df)+len(r_df)))*100))\nprint(\"Percentage of N2 stage:\",((len(n2_df)/(len(w_df)+len(n1_df)+len(n2_df)+len(n3_df)+len(r_df)))*100))\nprint(\"Percentage of N3 stage:\",((len(n3_df)/(len(w_df)+len(n1_df)+len(n2_df)+len(n3_df)+len(r_df)))*100))\nprint(\"Percentage of R stage:\",((len(r_df)/(len(w_df)+len(n1_df)+len(n2_df)+len(n3_df)+len(r_df)))*100))","metadata":{"execution":{"iopub.status.busy":"2024-11-16T02:34:58.331542Z","iopub.execute_input":"2024-11-16T02:34:58.332552Z","iopub.status.idle":"2024-11-16T02:34:58.340844Z","shell.execute_reply.started":"2024-11-16T02:34:58.332516Z","shell.execute_reply":"2024-11-16T02:34:58.339723Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(n3_df),n3_df.columns","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:56:29.056968Z","iopub.execute_input":"2024-10-27T10:56:29.057401Z","iopub.status.idle":"2024-10-27T10:56:29.066752Z","shell.execute_reply.started":"2024-10-27T10:56:29.057369Z","shell.execute_reply":"2024-10-27T10:56:29.065198Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# w_df = w_df.drop(\"EtCO2\", axis='columns')\n# n1_df = n1_df.drop(\"EtCO2\", axis='columns')\n# n2_df = n2_df.drop(\"EtCO2\", axis='columns')\n# n3_df = n3_df.drop(\"EtCO2\", axis='columns')\n# r_df = r_df.drop(\"EtCO2\", axis='columns')","metadata":{"execution":{"iopub.status.busy":"2024-10-20T13:10:26.293753Z","iopub.execute_input":"2024-10-20T13:10:26.294613Z","iopub.status.idle":"2024-10-20T13:10:26.29944Z","shell.execute_reply.started":"2024-10-20T13:10:26.294559Z","shell.execute_reply":"2024-10-20T13:10:26.29819Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape\ndf.columns","metadata":{"execution":{"iopub.status.busy":"2024-10-27T17:30:58.773509Z","iopub.execute_input":"2024-10-27T17:30:58.773909Z","iopub.status.idle":"2024-10-27T17:30:58.780237Z","shell.execute_reply.started":"2024-10-27T17:30:58.773877Z","shell.execute_reply":"2024-10-27T17:30:58.779297Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Stores energy distribution plot in a folder","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pywt\nfrom tqdm import tqdm  # For progress bar\nfrom sklearn.metrics import classification_report  # For classification report\nimport matplotlib.pyplot as plt\nimport os\nimport zipfile\n\n# Assuming df is your DataFrame with columns for each channel and an 'anomalies' label column (0 = non-anomalous, 1 = anomalous)\nchannels = ['EEG CZ-O1']  # Adjust channels as needed\nsegment_length = 1280  # Define segment length\nsampling_frequency = 256  # Adjust according to your data\n\n# Define function for Wavelet Packet Decomposition (WPD)\ndef wavelet_packet_decomposition(signal, wavelet='db1', maxlevel=5):\n    wp = pywt.WaveletPacket(data=signal, wavelet=wavelet, maxlevel=maxlevel)\n    nodes = [node.path for node in wp.get_level(maxlevel, 'freq')]  # Get paths at maxlevel\n    features = {node: wp[node].data for node in nodes}\n    return features\n\n# Function to extract fixed-size segments from the dataset\ndef extract_segments(df, segment_length):\n    num_segments = len(df) // segment_length\n    segments = []\n    labels = []\n    \n    for i in range(num_segments):\n        segment = df.iloc[i * segment_length: (i + 1) * segment_length]\n        if len(segment) == segment_length:\n            segments.append(segment)\n            # Calculate anomaly label for the segment based on the 50% threshold\n            anomaly_count = segment['anomalies'].sum()\n            segment_label = 1 if (anomaly_count / segment_length) >= 0.5 else 0\n            labels.append(segment_label)\n\n    return segments, labels\n\n# Get segments and corresponding labels for evaluation\nsegments, true_labels = extract_segments(df, segment_length=segment_length)\n\n# Analyze each segment with WPD and assign a label based on statistical characteristics\npredicted_labels = []\nenergy_values_all_segments = []  # To store energy values for dynamic thresholding\n\n# Process each segment with a progress bar\nfor segment in tqdm(segments, desc=\"Processing Segments\"):\n    segment_result = {}\n    is_anomaly = False\n\n    for channel in channels:\n        # Apply WPD on each channel for the current segment\n        signal = segment[channel].values  # Convert to array for computation\n        wpd_features = wavelet_packet_decomposition(signal, wavelet='db4', maxlevel=5)\n\n        # Calculate energy for each node in the wavelet packet tree\n        energy = {node: np.sum(np.abs(coeff)**2) for node, coeff in wpd_features.items()}\n        energy_values = list(energy.values())\n        energy_values_all_segments.append(energy_values)  # Collect energy values\n\n        # Calculate the mean and standard deviation for the current segment's energy values\n        mean_energy = np.mean(energy_values)\n        std_energy = np.std(energy_values)\n        \n        # Dynamic threshold based on mean and increased sensitivity\n        dynamic_threshold = mean_energy + 2 * std_energy  # Increased multiplier\n        \n        # Calculate range of energy values\n        energy_range = max(energy_values) - min(energy_values)  # Calculate range of energy values\n        \n        if energy_range > dynamic_threshold:  # If range exceeds dynamic threshold, classify as anomaly\n            is_anomaly = True\n\n    # Append predicted label based on fluctuations\n    predicted_labels.append(1 if is_anomaly else 0)\n\n# Evaluation against ground truth labels\naccuracy = np.mean(np.array(predicted_labels) == np.array(true_labels))\nprint(f\"\\nClassification Accuracy: {accuracy * 100:.2f}%\")\n\n# Generate and print classification report\nprint(\"\\nClassification Report:\")\nprint(classification_report(true_labels, predicted_labels, target_names=[\"Non-anomalous\", \"Anomalous\"]))\n\n# Count and print correct classifications\ncorrectly_classified_non_anomalies = np.sum((true_labels == 0) & (predicted_labels == 0))\ncorrectly_classified_anomalies = np.sum((true_labels == 1) & (predicted_labels == 1))\n\ntotal_non_anomalies = np.sum(true_labels == 0)\ntotal_anomalies = np.sum(true_labels == 1)\n\nprint(f\"\\nCorrectly classified non-anomalies: {correctly_classified_non_anomalies}\")\nprint(f\"Correctly classified anomalies: {correctly_classified_anomalies}\")\nprint(f\"Total non-anomalies: {total_non_anomalies}\")\nprint(f\"Total anomalies: {total_anomalies}\")\n\n# Create directories for saving plots\nos.makedirs('plots/anomalous', exist_ok=True)\nos.makedirs('plots/non_anomalous', exist_ok=True)\n\n# Function to plot energy distributions for each segment\ndef plot_energy_distribution(energy_values, title, idx, base_path):\n    plt.figure(figsize=(10, 6))\n    plt.plot(energy_values, marker='o', linestyle='-', alpha=0.7)\n    plt.title(f\"{title} Energy Distribution - Segment {idx + 1}\")\n    plt.xlabel(\"Wavelet Packet Nodes\")\n    plt.ylabel(\"Energy\")\n    plt.xticks(rotation=45)\n    plt.grid()\n    \n    # Save the plot as a PNG file\n    plt.savefig(f\"{base_path}/{title}_Segment_{idx + 1}.png\")\n    plt.close()  # Close the figure to avoid memory issues\n\n# Process each segment and plot the energy distributions based on ground truth\nfor idx, (segment, true_label) in enumerate(zip(segments, true_labels)):\n    # Apply WPD on each channel for the current segment\n    channel_energy = []\n    \n    for channel in channels:\n        signal = segment[channel].values  # Convert to array for computation\n        wpd_features = wavelet_packet_decomposition(signal, wavelet='db4', maxlevel=5)\n        \n        # Calculate energy for each node in the wavelet packet tree\n        energy = {node: np.sum(np.abs(coeff)**2) for node, coeff in wpd_features.items()}\n        channel_energy.append(list(energy.values()))\n\n    # Plot energy distribution based on ground truth labels\n    if true_label == 1:  # Anomalous\n        plot_energy_distribution(channel_energy[0], \"Anomalous\", idx, 'plots/anomalous')\n    else:  # Non-anomalous\n        plot_energy_distribution(channel_energy[0], \"Non-Anomalous\", idx, 'plots/non_anomalous')\n\n# Create a ZIP file of the saved plots\nwith zipfile.ZipFile('10000_17728_CZO1_plots.zip', 'w') as zipf:\n    for root, dirs, files in os.walk('plots'):\n        for file in files:\n            zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), os.path.join('plots', '..')))\n\n# Now you can download the 'plots.zip' file from your working directory.","metadata":{"execution":{"iopub.status.busy":"2024-10-28T10:25:46.730813Z","iopub.execute_input":"2024-10-28T10:25:46.731676Z","iopub.status.idle":"2024-10-28T10:45:51.088745Z","shell.execute_reply.started":"2024-10-28T10:25:46.731642Z","shell.execute_reply":"2024-10-28T10:45:51.087936Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a ZIP file of the saved plots\nwith zipfile.ZipFile('plots.zip', 'w') as zipf:\n    for root, dirs, files in os.walk('plots'):\n        for file in files:\n            zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), os.path.join('plots', '..')))","metadata":{"execution":{"iopub.status.busy":"2024-10-28T07:57:34.472343Z","iopub.execute_input":"2024-10-28T07:57:34.473253Z","iopub.status.idle":"2024-10-28T07:57:36.032953Z","shell.execute_reply.started":"2024-10-28T07:57:34.47322Z","shell.execute_reply":"2024-10-28T07:57:36.032167Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\n\n# Replace 'your_directory_path' with the path of the directory you want to delete\ndirectory_path = '/kaggle/working/plots.zip'\n\n# Check if the directory exists\nif os.path.exists(directory_path):\n    # Delete the directory and all its contents\n    shutil.rmtree(directory_path)\n    print(f\"Deleted directory: {directory_path}\")\nelse:\n    print(\"Directory does not exist.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-28T10:17:23.416447Z","iopub.execute_input":"2024-10-28T10:17:23.41711Z","iopub.status.idle":"2024-10-28T10:17:23.495107Z","shell.execute_reply.started":"2024-10-28T10:17:23.417078Z","shell.execute_reply":"2024-10-28T10:17:23.4937Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2024-10-29T10:26:23.043899Z","iopub.execute_input":"2024-10-29T10:26:23.044581Z","iopub.status.idle":"2024-10-29T10:26:23.050572Z","shell.execute_reply.started":"2024-10-29T10:26:23.044549Z","shell.execute_reply":"2024-10-29T10:26:23.049678Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualization","metadata":{}},{"cell_type":"markdown","source":"# Visualization (py.wavedec)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pywt\nfrom scipy.stats import entropy, kurtosis\nfrom scipy.fft import fft, fftfreq\nfrom tqdm import tqdm\n\n# Parameters and Data\nchannels = ['EEG F4-M1']  # Adjust channels as needed\nsegment_length = 1280  # Define segment length\nsampling_frequency = 256  # Adjust according to your data\nwavelet = 'db4'  # Define wavelet for decomposition\nlevel = 5  # Define the maximum decomposition level\n\n# Assuming df is your DataFrame with EEG channels and an 'anomalies' column (0 = non-anomalous, 1 = anomalous)\n\n# Function to extract segments from the dataset\ndef extract_segments(df, segment_length):\n    num_segments = len(df) // segment_length\n    segments = []\n    labels = []\n    \n    for i in range(num_segments):\n        segment = df.iloc[i * segment_length: (i + 1) * segment_length]\n        if len(segment) == segment_length:\n            segments.append(segment)\n            # Calculate anomaly label for the segment based on the 50% threshold\n            anomaly_count = segment['anomalies'].sum()\n            segment_label = 1 if (anomaly_count / segment_length) >= 0.5 else 0\n            labels.append(segment_label)\n    \n    return segments, labels\n\n# Extract segments and labels\nsegments, true_labels = extract_segments(df, segment_length=segment_length)\n\n# Store feature values for each segment by level\nsegment_energy_values = []\n\n# Process each segment with Wavelet Decomposition\nfor segment in tqdm(segments, desc=\"Processing Segments\"):\n    channel_feature_values = {}\n    for channel in channels:\n        # Apply Wavelet Decomposition on each channel for the current segment\n        signal = segment[channel].values  # Convert to array for computation\n        coeffs = pywt.wavedec(signal, wavelet, level=level)\n        \n        # Iterate over each level's coefficients and calculate features\n        for i, coeff in enumerate(coeffs):\n            # Energy of coefficients\n            energy = np.sum(np.abs(coeff) ** 2)\n            channel_feature_values[f\"level_{i}_energy\"] = energy\n            \n            \n            # Mean value\n#             mean_value = np.mean(np.abs(coeff))\n#             channel_feature_values[f\"level_{i}_mean\"] = mean_value\n            \n#             # Standard deviation\n#             std_value = np.std(coeff)\n#             channel_feature_values[f\"level_{i}_std\"] = std_value\n            \n#             # Entropy\n#             prob_dist = np.abs(coeff) / np.sum(np.abs(coeff))\n#             entropy_value = entropy(prob_dist)\n#             channel_feature_values[f\"level_{i}_entropy\"] = entropy_value\n            \n#             # FFT and Frequency Features\n#             coeff_fft = np.abs(fft(coeff))\n#             freqs = fftfreq(len(coeff), d=1/sampling_frequency)\n            \n#             # Dominant frequency (frequency with max FFT magnitude)\n#             dominant_freq = freqs[np.argmax(coeff_fft)]\n#             channel_feature_values[f\"level_{i}_dominant_freq\"] = dominant_freq\n            \n#             # Kurtosis\n#             kurtosis_value = kurtosis(coeff)\n#             channel_feature_values[f\"level_{i}_kurtosis\"] = kurtosis_value\n            \n#             # Highest frequency\n#             highest_freq = np.max(freqs)\n#             channel_feature_values[f\"level_{i}_highest_freq\"] = highest_freq\n\n    # Append feature values for the current segment\n    segment_energy_values.append(channel_feature_values)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T04:46:22.154431Z","iopub.execute_input":"2024-10-30T04:46:22.154934Z","iopub.status.idle":"2024-10-30T04:46:24.573455Z","shell.execute_reply.started":"2024-10-30T04:46:22.154899Z","shell.execute_reply":"2024-10-30T04:46:24.572577Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot both anomalous and non-anomalous segments on a single plot\nplt.figure(figsize=(12, 8))\n\n# Plot non-anomalous segments with lower transparency and thinner lines\nfor idx, energy_dict in enumerate(segment_energy_values):\n    energies = list(energy_dict.values())\n    nodes = range(1,len(energies))\n#     nodes = [16,17] #range(2,len(energies))\n    if true_labels[idx] == 0:  # Non-Anomalous\n        plt.plot(nodes, energies[1:], color='blue', alpha=0.2, linewidth=0.5)\n#         plt.plot(nodes, energies[16:18], color='blue', alpha=0.05, linewidth=0.5)  # Light blue for non-anomalous\n\n# Plot anomalous segments with higher opacity and thicker lines\nfor idx, energy_dict in enumerate(segment_energy_values):\n    energies = list(energy_dict.values())\n    nodes = range(1,len(energies))\n    if true_labels[idx] == 1:  # Anomalous\n        plt.plot(nodes, energies[1:], color='red', alpha=0.2, linewidth=0.6)  # Brighter red for anomalous\n\n# Labeling the graph\nplt.title(\"Energy vs. Nodes for Anomalous and Non-Anomalous Segments\")\nplt.xlabel(\"Wavelet Packet Nodes\")\nplt.ylabel(\"Energy Value\")\nplt.legend([\"Non-Anomalous Segments\", \"Anomalous Segments\"], loc=\"upper right\")\n\n# Display the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-30T08:45:20.396058Z","iopub.execute_input":"2024-10-30T08:45:20.396576Z","iopub.status.idle":"2024-10-30T08:45:27.990219Z","shell.execute_reply.started":"2024-10-30T08:45:20.396546Z","shell.execute_reply":"2024-10-30T08:45:27.989196Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"minimum1 = 10\nminimum2 = 10\nmaximum1 = -10\nmaximum2 = -10\nval = 16\nfor idx, energy_dict in enumerate(segment_energy_values):\n    energies = list(energy_dict.values())\n    nodes = range(len(energies))\n    if true_labels[idx] == 0:\n        minimum1 = min(minimum1,energies[val])\n        maximum1 = max(maximum1,energies[val])\n    elif true_labels[idx] == 1:\n        minimum2 = min(minimum2,energies[val])\n        maximum2 = max(maximum2,energies[val])\nprint(\"non anomalies = \",minimum1,\" \",maximum1)\nprint(\"anomalies = \",minimum2,\" \",maximum2)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T04:48:30.413841Z","iopub.execute_input":"2024-10-30T04:48:30.414607Z","iopub.status.idle":"2024-10-30T04:48:30.418999Z","shell.execute_reply.started":"2024-10-30T04:48:30.414575Z","shell.execute_reply":"2024-10-30T04:48:30.417954Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cnt = 0\ncnt2 =0\nprint(len(true_labels))\nfor i in true_labels:\n    if i == 1:\n        cnt += 1\n    else:\n        cnt2 += 1\nprint(cnt,\"percentage of anomalies in data = \",cnt/cnt2*100,\" %\")","metadata":{"execution":{"iopub.status.busy":"2024-10-29T18:17:55.08126Z","iopub.execute_input":"2024-10-29T18:17:55.081604Z","iopub.status.idle":"2024-10-29T18:17:55.088634Z","shell.execute_reply.started":"2024-10-29T18:17:55.081578Z","shell.execute_reply":"2024-10-29T18:17:55.087779Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cnt = 0\ncnt2 = 0\ntotal = 0\n# val = 16\nfor idx, energy_dict in enumerate(segment_energy_values):\n    energies = list(energy_dict.values())\n    nodes = range(len(energies))\n    if energies[val] <= maximum2 and energies[val] >= minimum2:\n        label = 1\n    else:\n        label = 0\n    if (true_labels[idx] == 1 and label == 1) or (true_labels[idx] == 0 and label == 0):\n            cnt += 1\n    total += 1\n    \n    if true_labels[idx] == 0 and energies[val] >= minimum2 and energies[val] <= maximum2:\n        cnt2+=1\nprint(\"accuracy = \",cnt/total)\nprint(\"cnt2 = \",cnt2)","metadata":{"execution":{"iopub.status.busy":"2024-10-29T18:17:59.176318Z","iopub.execute_input":"2024-10-29T18:17:59.176905Z","iopub.status.idle":"2024-10-29T18:17:59.206575Z","shell.execute_reply.started":"2024-10-29T18:17:59.176874Z","shell.execute_reply":"2024-10-29T18:17:59.205656Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Wavelet","metadata":{}},{"cell_type":"markdown","source":"## Wavelet signature","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pywt\nimport matplotlib.pyplot as plt\nfrom scipy.fft import fft\nfrom scipy.stats import entropy\n\ndef spectral_entropy(coeff):\n    # Perform Fourier Transform\n    fft_vals = np.abs(fft(coeff))  # Take absolute values to get magnitudes\n\n    # Normalize to create a probability distribution (power spectrum)\n    power_spectrum = fft_vals**2\n    power_spectrum /= np.sum(power_spectrum)  # Normalize to create probabilities\n    \n    # Calculate the spectral entropy\n    spec_entropy = entropy(power_spectrum)\n    \n    return spec_entropy\n\n# Assuming you have a DataFrame `df` with columns for each channel and an 'anomaly' label column (0 = non-anomalous, 1 = anomalous)\n# Example: df = pd.read_csv('your_psg_data.csv')\n\n# Define the channels to analyze and sampling frequency\n# channels = ['ECG EKG2-EKG']  # Select relevant channels\nchannels = ['EMG Chin2-Chin1'] # ,'EEG F4-M1','EEG C3-M2', 'EEG C4-M1', 'EEG O1-M2', 'EEG O2-M1', 'EEG CZ-O1\nsampling_frequency = 256  # Adjust according to your data\n\n# Define function for Wavelet Packet Decomposition (WPD)\ndef wavelet_packet_decomposition(signal, wavelet='db1', maxlevel=3):\n    wp = pywt.WaveletPacket(data=signal, wavelet=wavelet, maxlevel=maxlevel)\n    nodes = [node.path for node in wp.get_level(maxlevel, 'freq')]  # Get paths at maxlevel\n    features = {node: wp[node].data for node in nodes}\n    return features\n\n# Segment data by anomaly label\nanomalous_segments = df[df['anomalies'] == 1]\nnon_anomalous_segments = df[df['anomalies'] == 0]\nn = 7\nnon_anomalous_segments = non_anomalous_segments.iloc[len(anomalous_segments)*n:len(anomalous_segments)*(n+1)]\nprint(len(non_anomalous_segments),len(anomalous_segments))\n# Analyze each segment\nanomalous_wpd_results = []\nnon_anomalous_wpd_results = []\n\nfor label, segments in [('Anomalous', anomalous_segments), ('Non-anomalous', non_anomalous_segments)]:\n#     for idx, row in segments.iterrows():\n    segment_result = {}\n\n    for channel in channels:\n        # Apply WPD on each channel for the current segment\n        signal = segments[channel]\n        segment_length = len(signal)\n        wpd_features = wavelet_packet_decomposition(signal, wavelet='db4', maxlevel=4)  # Example: 'db4', level 4\n\n        # Calculate energy for each node in the wavelet packet tree\n        energy = {node: np.sum(np.abs(coeff)**2) for node, coeff in wpd_features.items()}\n#         normalized_energy = {node: np.sum(np.abs(coeff)**2) / segment_length for node, coeff in wpd_features.items()}\n        segment_result[f\"{channel}_energy\"] = energy # energy\n#         spec_entropy = {node: np.sum(spectral_entropy(coeff)) / segment_length for node, coeff in wpd_features.items()}\n#         segment_result[f\"{channel}_spec_entropy\"] = normalized_energy # energy\n\n    # Store results based on anomaly label\n    if label == 'Anomalous':\n        anomalous_wpd_results.append(segment_result)\n    else:\n        non_anomalous_wpd_results.append(segment_result)\n\ndef plot_energy_distribution(anomalous_results, non_anomalous_results, title):\n    for channel in channels:\n        plt.figure(figsize=(10, 6))\n        \n        # Plot anomalous segments\n        for result in anomalous_results:\n            energy_values = list(result[f\"{channel}_energy\"].values())\n            plt.plot(energy_values, color='red', alpha=0.5, label='Anomalous' if result == anomalous_results[0] else \"\")\n\n        # Plot non-anomalous segments\n        for result in non_anomalous_results:\n            energy_values = list(result[f\"{channel}_energy\"].values())\n            plt.plot(energy_values, color='blue', alpha=0.5, label='Non-anomalous' if result == non_anomalous_results[0] else \"\")\n\n        plt.title(f\"{title} - Energy Distribution for {channel}\")\n        plt.xlabel(\"Wavelet Packet Nodes\")\n        plt.ylabel(\"Energy\")\n        plt.legend()\n        plt.show()\n\n# Visualize energy distributions for anomalous and non-anomalous segments on the same plot\nplot_energy_distribution(anomalous_wpd_results, non_anomalous_wpd_results, title=\"Energy Distribution of Segments\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T17:33:27.879596Z","iopub.execute_input":"2024-10-27T17:33:27.880363Z","iopub.status.idle":"2024-10-27T17:33:28.965697Z","shell.execute_reply.started":"2024-10-27T17:33:27.880332Z","shell.execute_reply":"2024-10-27T17:33:28.964811Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Wavelet (pywt.swt)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pywt\n\n# Example DataFrame for reference\n# Assuming df['SpO2'] has the SpO2 values and df['anomalies'] contains the anomaly labels\n# df = pd.DataFrame({\n#     'EEG F3-M2': np.random.normal(95, 1, 1000),\n#     'EEG F4-M1': np.random.normal(95, 1, 1000),\n#     'EEG C3-M2': np.random.normal(95, 1, 1000),\n#     'anomalies': np.random.randint(0, 2, 1000)\n# })\n\n# Wavelet Transform and Feature Extraction for a single channel\ndef extract_wavelet_features(channel_data, wavelet='db4', level=4):\n    \"\"\"\n    Extract wavelet features from a signal using Stationary Wavelet Transform (SWT).\n    \n    :param channel_data: Array of signal values.\n    :param wavelet: Wavelet type (e.g., 'db4').\n    :param level: Decomposition level.\n    :return: List of wavelet features.\n    \"\"\"\n    # Perform Stationary Wavelet Transform (SWT)\n    coeffs = pywt.swt(channel_data, wavelet, level=level)\n    \n    # Extract features from approximation and detail coefficients at each level\n    features = []\n    for cA, cD in coeffs:  # Separate approximation and detail coefficients\n        # Extract features from detail coefficients (cD)\n        features.append(np.mean(cD))   # Mean of detail coefficients\n        features.append(np.std(cD))    # Standard deviation of detail coefficients\n        features.append(np.sum(cD**2)) # Energy (sum of squared detail coefficients)\n        \n    return features\n\n# Apply wavelet transform to each EEG segment for each channel\nsegment_length = 1280 #2560  # Example for 10 seconds with 256 Hz sampling rate\nwavelet_features = []\n\nfor i in range(0, len(df), segment_length):\n    segment = df[['EMG Chin2-Chin1']].iloc[i:i+segment_length]\n    \n    if len(segment) == segment_length:  # Ensure the segment is complete\n        channel_features = []\n        \n        # Extract features for each channel independently\n        for channel in segment.columns:\n            channel_data = segment[channel].values\n            features = extract_wavelet_features(channel_data)\n            channel_features.extend(features)  # Add features for this channel\n            \n        wavelet_features.append(channel_features)\n\n# Create column names to reflect multiple channels and features\nnum_features_per_channel = len(wavelet_features[0]) // len(segment.columns)\ncolumns = [f'{col}_feature_{i}' for col in segment.columns for i in range(num_features_per_channel)]\n\n# Convert wavelet features into a DataFrame with appropriate column names\nwavelet_features_df = pd.DataFrame(wavelet_features, columns=columns)\n\n# Aligning with anomaly labels: \n# For each segment, assign the corresponding anomaly label (if more than 20% of the rows contain anomalies)\nanomaly_labels = []\n\nfor i in range(0, len(df), segment_length):\n    segment_anomalies = df['anomalies'].iloc[i:i+segment_length].values\n    \n    if len(segment_anomalies) == segment_length:\n        # Label the segment as an anomaly if more than 20% of the rows are anomalies\n        if np.sum(segment_anomalies) > 0.5 * segment_length:\n            anomaly_labels.append(1)\n        else:\n            anomaly_labels.append(0)\n\n# Add the anomaly labels to the wavelet feature DataFrame\nwavelet_features_df['anomalies'] = anomaly_labels\n\n# Show the final DataFrame with wavelet features and anomaly labels\nprint(wavelet_features_df.head())","metadata":{"execution":{"iopub.status.busy":"2024-10-27T17:49:06.754819Z","iopub.execute_input":"2024-10-27T17:49:06.755195Z","iopub.status.idle":"2024-10-27T17:49:06.889162Z","shell.execute_reply.started":"2024-10-27T17:49:06.755154Z","shell.execute_reply":"2024-10-27T17:49:06.887954Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2024-11-02T10:31:41.442025Z","iopub.execute_input":"2024-11-02T10:31:41.442707Z","iopub.status.idle":"2024-11-02T10:31:41.449645Z","shell.execute_reply.started":"2024-11-02T10:31:41.442673Z","shell.execute_reply":"2024-11-02T10:31:41.448666Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Wavelet (pywt.wavedec)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pywt\nfrom scipy.fft import fft, fftfreq\nfrom scipy.stats import entropy, skew, kurtosis, median_abs_deviation, iqr\nfrom scipy.signal import hilbert\n\ndef rms(coeff):\n    return np.sqrt(np.mean(coeff**2))\n\ndef peak_to_peak(coeff):\n    return np.ptp(coeff)\n\ndef mean_absolute_deviation(coeff):\n    return median_abs_deviation(coeff)\n\ndef interquartile_range(coeff):\n    return iqr(coeff)\n\ndef zero_crossing_rate(coeff):\n    return ((coeff[:-1] * coeff[1:]) < 0).sum() / len(coeff)\n\ndef slope(coeff):\n    return np.polyfit(range(len(coeff)), coeff, 1)[0]\n\ndef hjorth_parameters(coeff):\n    # Hjorth parameters (Activity, Mobility, Complexity)\n    diff1 = np.diff(coeff)\n    diff2 = np.diff(diff1)\n    activity = np.var(coeff)\n    mobility = np.sqrt(np.var(diff1) / activity)\n    complexity = np.sqrt(np.var(diff2) / np.var(diff1)) / mobility\n    return activity, mobility, complexity\n\ndef autocorrelation(coeff, lag=1):\n    return np.corrcoef(coeff[:-lag], coeff[lag:])[0, 1]\n\ndef teager_kaiser_energy(coeff):\n    return np.sum(coeff[1:-1]**2 - coeff[:-2] * coeff[2:])\n\ndef spectral_entropy(coeff):\n    # Perform Fourier Transform\n    fft_vals = np.abs(fft(coeff))  # Take absolute values to get magnitudes\n\n    # Normalize to create a probability distribution (power spectrum)\n    power_spectrum = fft_vals**2\n    power_spectrum /= np.sum(power_spectrum)  # Normalize to create probabilities\n    \n    # Calculate the spectral entropy\n    spec_entropy = entropy(power_spectrum)\n    \n    return spec_entropy\n\ndef dominant_frequency(coeff, sampling_rate=256):\n    # Perform Fourier Transform\n    freqs = fftfreq(len(coeff), 1 / sampling_rate)  # Frequency values\n    fft_vals = fft(coeff)\n    \n    # Compute power spectrum (magnitude of fft values)\n    power_spectrum = np.abs(fft_vals)**2\n\n    # Find the frequency with the maximum power\n    dom_freq = freqs[np.argmax(power_spectrum)]\n    \n    return dom_freq\n\ndef median_frequency(coeff, sampling_rate=256):\n    freqs = fftfreq(len(coeff), 1 / sampling_rate)\n    fft_vals = np.abs(fft(coeff)) ** 2\n    cumulative_power = np.cumsum(fft_vals)\n    total_power = cumulative_power[-1]\n    return freqs[np.where(cumulative_power >= total_power / 2)[0][0]]\n\ndef bandwidth(coeff, sampling_rate=256):\n    freqs = fftfreq(len(coeff), 1 / sampling_rate)\n    fft_vals = np.abs(fft(coeff)) ** 2\n    mean_freq = np.sum(freqs * fft_vals) / np.sum(fft_vals)\n    return np.sqrt(np.sum(((freqs - mean_freq) ** 2) * fft_vals) / np.sum(fft_vals))\n\ndef spectral_skewness(coeff):\n    fft_vals = np.abs(fft(coeff)) ** 2\n    return skew(fft_vals)\n\ndef spectral_kurtosis(coeff):\n    fft_vals = np.abs(fft(coeff)) ** 2\n    return kurtosis(fft_vals)\n\ndef peak_frequency(coeff, sampling_rate=256):\n    freqs = fftfreq(len(coeff), 1 / sampling_rate)\n    fft_vals = np.abs(fft(coeff)) ** 2\n    sorted_indices = np.argsort(fft_vals)[::-1]\n    return freqs[sorted_indices[1]]\n\n\n# Wavelet Transform and Feature Extraction for a single channel\ndef extract_wavelet_features(spo2_data, wavelet='db4', level=4):\n    \"\"\"\n    Extract wavelet features from the SpO2 column using Discrete Wavelet Transform (DWT).\n    \n    :param spo2_data: Array of SpO2 values.\n    :param wavelet: Wavelet type (e.g., 'db4').\n    :param level: Decomposition level.\n    :return: List of wavelet features.\n    \"\"\"\n    # Perform discrete wavelet transform (DWT)\n#     coeffs = pywt.WaveletPacket(data=spo2_data, wavelet=wavelet, mode='symmetric', maxlevel=level)\n    coeffs = pywt.wavedec(spo2_data, wavelet, level=level)\n    \n    # Extract features from coefficients (e.g., mean, std, energy of each detail level)\n    features = []\n    for i, coeff in enumerate(coeffs):\n        features.append(np.mean(coeff))   # Mean of the coefficients\n        features.append(np.std(coeff))    # Standard deviation of the coefficients\n        features.append(np.sum(coeff**2)) # Energy (sum of squared coefficients)\n        \n        # features.append(rms(coeff))\n        # features.append(peak_to_peak(coeff))\n        # features.append(mean_absolute_deviation(coeff))\n        # features.append(interquartile_range(coeff))\n#         features.append(skew(coeff))\n#         features.append(kurtosis(coeff))\n        # features.append(zero_crossing_rate(coeff))\n        # features.append(slope(coeff))\n#         activity, mobility, complexity = hjorth_parameters(coeff)\n#         features.extend([activity, mobility, complexity])\n#         features.append(autocorrelation(coeff))\n        # features.append(teager_kaiser_energy(coeff))\n        \n        features.append((dominant_frequency(coeff, sampling_rate=256)))\n        features.append(spectral_entropy(coeff))\n        features.append(median_frequency(coeff))\n        features.append(bandwidth(coeff))\n        features.append(spectral_skewness(coeff))\n        features.append(spectral_kurtosis(coeff))\n        features.append(peak_frequency(coeff))\n        \n    return features\n\n# Apply wavelet transform to each SpO2 segment for each channel\nsegment_length = 1280 # 2560/2  # Example for 10 seconds with 256 Hz sampling rate\nwavelet_features = []\n\n# 'EOG LOC-M2', 'EOG ROC-M1', 'EMG Chin2-Chin1', 'EEG F3-M2', 'EEG F4-M1','EEG C3-M2', 'EEG C4-M1', 'EEG O1-M2', 'EEG O2-M1', 'EEG CZ-O1','EMG LLeg-RLeg', 'ECG EKG2-EKG'\n\nfor i in range(0, len(df), segment_length):\n    segment = df[['EOG LOC-M2', 'EOG ROC-M1', 'EEG F3-M2', 'EEG F4-M1','EEG C3-M2', 'EEG C4-M1', 'EEG O1-M2', 'EEG O2-M1', 'EEG CZ-O1','EMG LLeg-RLeg', 'ECG EKG2-EKG']].iloc[i:i+segment_length] # ,'EMG LLeg-RLeg','EEG F3-M2', 'EEG F4-M1','EEG C3-M2', 'EEG C4-M1', 'EEG O1-M2', 'EEG O2-M1', 'EEG CZ-O1'\n#     print(\"segment - \",segment)\n    if len(segment) == segment_length:  # Ensure the segment is complete\n        channel_features = []\n        \n        # Extract features for each channel independently\n        for channel in segment.columns:\n            channel_data = segment[channel].values\n            features = extract_wavelet_features(channel_data)\n            channel_features.extend(features)  # Add features for this channel\n            \n        wavelet_features.append(channel_features)\n\n# Create column names to reflect multiple channels and features\nnum_features_per_channel = len(wavelet_features[0]) // len(segment.columns)\ncolumns = [f'{col}_feature_{i}' for col in segment.columns for i in range(num_features_per_channel)]\n\n# Convert wavelet features into a DataFrame with appropriate column names\nwavelet_features_df = pd.DataFrame(wavelet_features, columns=columns)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T14:52:00.746628Z","iopub.execute_input":"2024-11-14T14:52:00.747011Z","iopub.status.idle":"2024-11-14T15:27:43.621242Z","shell.execute_reply.started":"2024-11-14T14:52:00.746983Z","shell.execute_reply":"2024-11-14T15:27:43.619831Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Aligning with anomaly labels: \n# For each segment, assign the corresponding anomaly label (if more than 20% of the rows contain anomalies)\nanomaly_labels = []\n\nfor i in range(0, len(df), segment_length):\n    segment_anomalies = df['anomalies'].iloc[i:i+segment_length].values\n    \n    if len(segment_anomalies) == segment_length:\n        # Label the segment as an anomaly if more than 20% of the rows are anomalies\n        if np.sum(segment_anomalies) > 0.5 * segment_length:\n            anomaly_labels.append(1)\n        else:\n            anomaly_labels.append(0)\n\n# Add the anomaly labels to the wavelet feature DataFrame\nwavelet_features_df['anomalies'] = anomaly_labels\n\n# Show the final DataFrame with wavelet features and anomaly labels\nprint(wavelet_features_df.head())\n\n# Assuming df is your DataFrame\noutput_path = '/kaggle/working/output.csv'\nwavelet_features_df.to_csv(output_path, index=False)\n\n# Display the file path to easily download\nprint(f\"File saved to: {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:07:14.977727Z","iopub.execute_input":"2024-11-14T16:07:14.978573Z","iopub.status.idle":"2024-11-14T16:07:22.325844Z","shell.execute_reply.started":"2024-11-14T16:07:14.978541Z","shell.execute_reply":"2024-11-14T16:07:22.324827Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# wavelet_features_df.shape\nprint(\"hello\",len(wavelet_features_df))\nwavelet_features_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-14T16:08:14.859911Z","iopub.execute_input":"2024-11-14T16:08:14.860516Z","iopub.status.idle":"2024-11-14T16:08:14.88575Z","shell.execute_reply.started":"2024-11-14T16:08:14.86048Z","shell.execute_reply":"2024-11-14T16:08:14.884912Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Wavelet (pywt.WaveletPacket)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pywt\nfrom scipy.fft import fft, fftfreq\nfrom scipy.stats import entropy, skew, kurtosis\n\ndef spectral_entropy(coeff):\n    # Perform Fourier Transform\n    fft_vals = np.abs(fft(coeff))  # Take absolute values to get magnitudes\n    power_spectrum = fft_vals**2\n    power_spectrum /= np.sum(power_spectrum)  # Normalize to create probabilities\n    spec_entropy = entropy(power_spectrum)\n    return spec_entropy\n\ndef dominant_frequency(coeff, sampling_rate=256):\n    freqs = fftfreq(len(coeff), 1 / sampling_rate)  # Frequency values\n    fft_vals = fft(coeff)\n    power_spectrum = np.abs(fft_vals)**2\n    dom_freq = freqs[np.argmax(power_spectrum)]\n    return dom_freq\n\ndef median_frequency(coeff, sampling_rate=256):\n    freqs = fftfreq(len(coeff), 1 / sampling_rate)\n    fft_vals = np.abs(fft(coeff)) ** 2\n    cumulative_power = np.cumsum(fft_vals)\n    total_power = cumulative_power[-1]\n    return freqs[np.where(cumulative_power >= total_power / 2)[0][0]]\n\ndef bandwidth(coeff, sampling_rate=256):\n    freqs = fftfreq(len(coeff), 1 / sampling_rate)\n    fft_vals = np.abs(fft(coeff)) ** 2\n    mean_freq = np.sum(freqs * fft_vals) / np.sum(fft_vals)\n    return np.sqrt(np.sum(((freqs - mean_freq) ** 2) * fft_vals) / np.sum(fft_vals))\n\ndef spectral_skewness(coeff):\n    fft_vals = np.abs(fft(coeff)) ** 2\n    return skew(fft_vals)\n\ndef spectral_kurtosis(coeff):\n    fft_vals = np.abs(fft(coeff)) ** 2\n    return kurtosis(fft_vals)\n\ndef peak_frequency(coeff, sampling_rate=256):\n    freqs = fftfreq(len(coeff), 1 / sampling_rate)\n    fft_vals = np.abs(fft(coeff)) ** 2\n    sorted_indices = np.argsort(fft_vals)[::-1]\n    return freqs[sorted_indices[1]]\n\n# Wavelet Transform and Feature Extraction for a single channel\ndef extract_wavelet_features(spo2_data, wavelet='db4', level=4):\n    \"\"\"\n    Extract wavelet features from the SpO2 column using Discrete Wavelet Transform (DWT).\n    \n    :param spo2_data: Array of SpO2 values.\n    :param wavelet: Wavelet type (e.g., 'db4').\n    :param level: Decomposition level.\n    :return: List of wavelet features.\n    \"\"\"\n    coeffs = pywt.WaveletPacket(data=spo2_data, wavelet=wavelet, mode='symmetric', maxlevel=level)\n    features = []\n    for node in coeffs.get_level(level, order='freq'):\n        coeff = node.data\n        features.append(np.mean(coeff))   # Mean of the coefficients\n        features.append(np.std(coeff))    # Standard deviation of the coefficients\n        features.append(np.sum(coeff**2)) # Energy (sum of squared coefficients)\n        features.append(dominant_frequency(coeff, sampling_rate=256))\n        features.append(median_frequency(coeff))\n        features.append(bandwidth(coeff))\n        features.append(spectral_skewness(coeff))\n        features.append(spectral_kurtosis(coeff))\n        features.append(peak_frequency(coeff))\n    return features\n\n# Apply wavelet transform to each SpO2 segment for each channel\nsegment_length = 1280  # Example for 10 seconds with 256 Hz sampling rate\nwavelet_features = []\n\n# Assume 'df' is your DataFrame with SpO2 and anomaly labels\nfor i in range(0, len(df), segment_length):\n    segment = df[['EOG ROC-M1', 'EMG Chin2-Chin1']].iloc[i:i+segment_length]  # Use selected columns, e.g., 'EOG LOC-M2','EEG F4-M1','EEG F3-M2'\n    if len(segment) == segment_length:  # Ensure the segment is complete\n        channel_features = []\n        \n        # Extract features for each channel independently\n        for channel in segment.columns:\n            channel_data = segment[channel].values\n            features = extract_wavelet_features(channel_data)\n            channel_features.extend(features)  # Add features for this channel\n            \n        wavelet_features.append(channel_features)\n\n# Create column names to reflect multiple channels and features\nnum_features_per_channel = len(wavelet_features[0]) // len(segment.columns)\ncolumns = [f'{col}_feature_{i}' for col in segment.columns for i in range(num_features_per_channel)]\n\n# Convert wavelet features into a DataFrame with appropriate column names\nwavelet_features_df = pd.DataFrame(wavelet_features, columns=columns)\n\n# Aligning with anomaly labels: \n# For each segment, assign the corresponding anomaly label (if more than 20% of the rows contain anomalies)\nanomaly_labels = []\n\nfor i in range(0, len(df), segment_length):\n    segment_anomalies = df['anomalies'].iloc[i:i+segment_length].values\n    if len(segment_anomalies) == segment_length:\n        # Label the segment as an anomaly if more than 20% of the rows are anomalies\n        if np.sum(segment_anomalies) > 0.5 * segment_length:\n            anomaly_labels.append(1)\n        else:\n            anomaly_labels.append(0)\n\n# Add the anomaly labels to the wavelet feature DataFrame\nwavelet_features_df['anomalies'] = anomaly_labels\n\n# Show the final DataFrame with wavelet features and anomaly labels\nprint(wavelet_features_df.head())","metadata":{"execution":{"iopub.status.busy":"2024-10-30T03:42:46.546582Z","iopub.execute_input":"2024-10-30T03:42:46.546967Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dual-Tree Complex Wavelet Transform","metadata":{}},{"cell_type":"code","source":"!pip install dtcwt","metadata":{"execution":{"iopub.status.busy":"2024-10-29T18:24:13.545921Z","iopub.execute_input":"2024-10-29T18:24:13.546583Z","iopub.status.idle":"2024-10-29T18:24:32.065285Z","shell.execute_reply.started":"2024-10-29T18:24:13.546547Z","shell.execute_reply":"2024-10-29T18:24:32.064223Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pywt\nfrom scipy.fft import fft, fftfreq\nfrom scipy.stats import entropy, skew, kurtosis\n\ndef spectral_entropy(coeff):\n    # Perform Fourier Transform\n    fft_vals = np.abs(fft(coeff))  # Take absolute values to get magnitudes\n    power_spectrum = fft_vals**2\n    power_spectrum /= np.sum(power_spectrum)  # Normalize to create probabilities\n    spec_entropy = entropy(power_spectrum)\n    return spec_entropy\n\ndef dominant_frequency(coeff, sampling_rate=256):\n    freqs = fftfreq(len(coeff), 1 / sampling_rate)  # Frequency values\n    fft_vals = fft(coeff)\n    power_spectrum = np.abs(fft_vals)**2\n    dom_freq = freqs[np.argmax(power_spectrum)]\n    return dom_freq\n\ndef median_frequency(coeff, sampling_rate=256):\n    freqs = fftfreq(len(coeff), 1 / sampling_rate)\n    fft_vals = np.abs(fft(coeff)) ** 2\n    cumulative_power = np.cumsum(fft_vals)\n    total_power = cumulative_power[-1]\n    return freqs[np.where(cumulative_power >= total_power / 2)[0][0]]\n\ndef bandwidth(coeff, sampling_rate=256):\n    freqs = fftfreq(len(coeff), 1 / sampling_rate)\n    fft_vals = np.abs(fft(coeff)) ** 2\n    mean_freq = np.sum(freqs * fft_vals) / np.sum(fft_vals)\n    return np.sqrt(np.sum(((freqs - mean_freq) ** 2) * fft_vals) / np.sum(fft_vals))\n\ndef spectral_skewness(coeff):\n    fft_vals = np.abs(fft(coeff)) ** 2\n    return skew(fft_vals)\n\ndef spectral_kurtosis(coeff):\n    fft_vals = np.abs(fft(coeff)) ** 2\n    return kurtosis(fft_vals)\n\ndef peak_frequency(coeff, sampling_rate=256):\n    freqs = fftfreq(len(coeff), 1 / sampling_rate)\n    fft_vals = np.abs(fft(coeff)) ** 2\n    sorted_indices = np.argsort(fft_vals)[::-1]\n    return freqs[sorted_indices[1]]","metadata":{"execution":{"iopub.status.busy":"2024-10-29T18:33:39.726164Z","iopub.execute_input":"2024-10-29T18:33:39.726871Z","iopub.status.idle":"2024-10-29T18:33:39.738852Z","shell.execute_reply.started":"2024-10-29T18:33:39.72684Z","shell.execute_reply":"2024-10-29T18:33:39.737968Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport dtcwt  # Dual-Tree Complex Wavelet Transform\n\n# Wavelet Transform and Feature Extraction\ndef extract_dtcwt_features(spo2_data, level=5):\n    \"\"\"\n    Extract DTCWT features from the SpO2 column to retain phase information.\n    \n    :param spo2_data: Array of SpO2 values.\n    :param level: Decomposition level.\n    :return: List of wavelet features including phase information.\n    \"\"\"\n    # Initialize the DTCWT transform\n    transform = dtcwt.Transform1d()\n\n    # Perform DTCWT to get complex coefficients\n    coeffs = transform.forward(spo2_data, nlevels=level)\n\n    # Extract features from coefficients\n    features = []\n    for i in range(1, level + 1):\n        # Extract magnitude and phase from real and imaginary parts\n        magnitude = np.abs(coeffs.highpasses[i - 1])\n        phase = np.angle(coeffs.highpasses[i - 1])\n        \n        # Append statistical features of magnitude and phase (e.g., mean, std, energy)\n        features.extend([np.mean(magnitude), np.std(magnitude), np.sum(magnitude**2)])  # Magnitude\n        features.append(dominant_frequency(magnitude, sampling_rate=256))\n        features.append(median_frequency(magnitude))\n        features.append(bandwidth(magnitude))\n        features.append(spectral_skewness(magnitude))\n        features.append(spectral_kurtosis(magnitude))\n        features.append(peak_frequency(magnitude))\n        features.extend([np.mean(phase), np.std(phase)])  # Phase\n    \n    return features\n\n# Apply DTCWT to each SpO2 segment\nsegment_length = 1280  # Example for 10 seconds with 256 Hz sampling rate\nwavelet_features = []\n\nfor i in range(0, len(df), segment_length):\n    segment = df['EEG F4-M1'].iloc[i:i+segment_length].values\n    \n    if len(segment) == segment_length:  # Ensure the segment is complete\n        features = extract_dtcwt_features(segment)\n        wavelet_features.append(features)\n\n# Convert wavelet features into a DataFrame\nwavelet_features_df = pd.DataFrame(wavelet_features, columns=[f'feature_{i}' for i in range(len(wavelet_features[0]))])\n\n# Aligning with anomaly labels\nanomaly_labels = []\n\nfor i in range(0, len(df), segment_length):\n    segment_anomalies = df['anomalies'].iloc[i:i+segment_length].values\n    \n    if len(segment_anomalies) == segment_length:\n        if np.sum(segment_anomalies) > 0.5 * segment_length:\n            anomaly_labels.append(1)\n        else:\n            anomaly_labels.append(0)\n\n# Add the anomaly labels to the wavelet feature DataFrame\nwavelet_features_df['anomalies'] = anomaly_labels\n\n# Show the final DataFrame with wavelet features and anomaly labels\nprint(wavelet_features_df.head())","metadata":{"execution":{"iopub.status.busy":"2024-10-29T18:34:27.419135Z","iopub.execute_input":"2024-10-29T18:34:27.419532Z","iopub.status.idle":"2024-10-29T18:36:08.707096Z","shell.execute_reply.started":"2024-10-29T18:34:27.419501Z","shell.execute_reply":"2024-10-29T18:36:08.706068Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Selection of desired nodes for RFC","metadata":{}},{"cell_type":"code","source":"# Assuming each node generates 9 features per channel\nfeatures_per_node = 9\nselected_nodes = range(1, 5)  # Nodes 4 to 10\nselected_columns = []\n\n# Loop through each channel and select columns for nodes 4 to 10\nfor channel in segment.columns:\n    for node in selected_nodes:\n        # Calculate the starting index for the current node\n        start_index = node * features_per_node\n        # Select columns corresponding to the features of this node\n        selected_columns.extend([f\"{channel}_feature_{i}\" for i in range(start_index, start_index + features_per_node)])\n\n# Create a DataFrame with only the selected features for nodes 4 to 10\nwavelet_features_df_selected = wavelet_features_df[selected_columns]\n\n# If you need to keep the anomaly label, add it back to the selected DataFrame\nwavelet_features_df_selected['anomalies'] = wavelet_features_df['anomalies']\n\n# Show the resulting DataFrame\nprint(wavelet_features_df_selected.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T18:04:23.151269Z","iopub.execute_input":"2024-10-31T18:04:23.152006Z","iopub.status.idle":"2024-10-31T18:04:23.176262Z","shell.execute_reply.started":"2024-10-31T18:04:23.15197Z","shell.execute_reply":"2024-10-31T18:04:23.175356Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"non_anomalies = wavelet_features_df[wavelet_features_df['anomalies'] == 0]\nprint(len(non_anomalies))\nanomalies = wavelet_features_df[wavelet_features_df['anomalies'] == 1]\nprint(len(anomalies))\nwavelet_features_df.shape\n# wavelet_features_df.columns[:50]","metadata":{"execution":{"iopub.status.busy":"2024-11-05T05:25:06.031308Z","iopub.execute_input":"2024-11-05T05:25:06.031941Z","iopub.status.idle":"2024-11-05T05:25:06.041666Z","shell.execute_reply.started":"2024-11-05T05:25:06.03191Z","shell.execute_reply":"2024-11-05T05:25:06.040784Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Separate the features and the anomalies column\nfeatures = wavelet_features_df.drop(columns=['anomalies'])\nanomalies = wavelet_features_df['anomalies']\n\n# Apply standardization to the feature columns\nscaler = StandardScaler()\nstandardized_features = scaler.fit_transform(features)\n\n# Convert the standardized features back into a DataFrame\nstandardized_features_df = pd.DataFrame(standardized_features, columns=features.columns)\n\n# Add the anomalies column back to the standardized DataFrame\nwavelet_features_df_standardized = pd.concat([standardized_features_df, anomalies.reset_index(drop=True)], axis=1)\n\n# Display the result\nprint(wavelet_features_df_standardized.head())","metadata":{"execution":{"iopub.status.busy":"2024-11-08T04:47:19.60269Z","iopub.execute_input":"2024-11-08T04:47:19.603055Z","iopub.status.idle":"2024-11-08T04:47:19.708103Z","shell.execute_reply.started":"2024-11-08T04:47:19.603025Z","shell.execute_reply":"2024-11-08T04:47:19.706925Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wavelet_features_df_standardized.head()\nprint(len(wavelet_features_df_standardized[wavelet_features_df_standardized['anomalies'] == 1]))\nlen(wavelet_features_df_standardized)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T04:47:21.091325Z","iopub.execute_input":"2024-11-08T04:47:21.092007Z","iopub.status.idle":"2024-11-08T04:47:21.100386Z","shell.execute_reply.started":"2024-11-08T04:47:21.091975Z","shell.execute_reply":"2024-11-08T04:47:21.099346Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## RFC for wavelet","metadata":{}},{"cell_type":"code","source":"scaled_n2_df.columns\nprint(len(scaled_n2_df[scaled_n2_df['anomalies'] == 1]))\nlen(scaled_n2_df[scaled_n2_df['anomalies'] == 0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T04:32:08.54756Z","iopub.execute_input":"2024-11-16T04:32:08.548531Z","iopub.status.idle":"2024-11-16T04:32:08.838475Z","shell.execute_reply.started":"2024-11-16T04:32:08.548483Z","shell.execute_reply":"2024-11-16T04:32:08.837487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df = scaled_w_df2\ntest_df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T04:32:14.992558Z","iopub.execute_input":"2024-11-16T04:32:14.992953Z","iopub.status.idle":"2024-11-16T04:32:15.001898Z","shell.execute_reply.started":"2024-11-16T04:32:14.992924Z","shell.execute_reply":"2024-11-16T04:32:15.000896Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import required libraries\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import balanced_accuracy_score, classification_report\n\n# Assuming wavelet_features_df is already defined and loaded with data\n# Separate anomalies and non-anomalies\n# anomalies = pca_features_df[pca_features_df['anomalies'] == 1]\n# non_anomalies = pca_features_df[pca_features_df['anomalies'] == 0]\n\n# anomalies = wavelet_features_df[wavelet_features_df['anomalies'] == 1]\n# non_anomalies = wavelet_features_df[wavelet_features_df['anomalies'] == 0]\n\n# anomalies = wavelet_features_df_selected[wavelet_features_df_selected['anomalies'] == 1]\n# non_anomalies = wavelet_features_df_selected[wavelet_features_df_selected['anomalies'] == 0]\n\n# anomalies = wavelet_features_df_standardized[wavelet_features_df_standardized['anomalies'] == 1]\n# non_anomalies = wavelet_features_df_standardized[wavelet_features_df_standardized['anomalies'] == 0]\ntest_df = scaled_n2_df\nanomalies = test_df[test_df['anomalies'] == 1]\nnon_anomalies = test_df[test_df['anomalies'] == 0]\n\nacc_list = []\n\n# val = 15\nfor val in range(5,100,10):\n    # Loop through n values from 250 to 400 in increments of 10\n    for n in range(len(anomalies)-1,len(anomalies), 10):\n        n = min(n,len(non_anomalies))\n        # Sample n anomalies and n non-anomalies\n        # anomalies_sample = anomalies.sample(n=n, random_state=val)\n        # non_anomalies_sample = non_anomalies.sample(n=n, random_state=val)\n\n        # Concatenate the sampled data to create a balanced dataset\n        balanced_data = pd.concat([anomalies_sample, non_anomalies_sample])\n\n        # Define features and target variable\n        # X = balanced_data.drop(columns=['anomalies'])\n        # y = balanced_data['anomalies']\n\n        X = test_df.drop(columns=['anomalies'])\n        y = test_df['anomalies']\n        \n        # Split the data into training and testing sets\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=val)\n        # split_percent = 0.60\n        # X_train = X.iloc[:int(len(X) * split_percent)]\n        # X_test = X.iloc[int(len(X) * split_percent):]\n        # y_train = y.iloc[:int(len(y) * split_percent)]\n        # y_test = y.iloc[int(len(y) * split_percent):]\n        print(len(X_train),len(X_test),len(y_train),len(y_test))\n        print(\"TRAIN SET: \", (y_train == 1).sum(), (y_train == 0).sum())\n        print(\"TEST SET: \", (y_test == 1).sum(), (y_test == 0).sum())\n\n        # Initialize the Random Forest Classifier\n        rfc = RandomForestClassifier(random_state=42)\n\n        # Train the model\n        rfc.fit(X_train, y_train)\n\n        # Predict on the test set\n        y_pred = rfc.predict(X_test)\n\n        # Calculate balanced accuracy\n        balanced_acc = balanced_accuracy_score(y_test, y_pred)\n        acc_list.append(balanced_acc)\n        print(f\"Balanced Accuracy for n={n}: {balanced_acc:.4f}\")\n\n        # Optional: Classification report\n        print(f\"Classification Report for n={n}:\\n\", classification_report(y_test, y_pred))\n        print(\"=\"*50)\n\n        # Identify misclassified samples\n        misclassified_indices = X_test[y_test != y_pred].index\n        misclassified_samples = X_test.loc[misclassified_indices]\n        misclassified_labels = y_test.loc[misclassified_indices]\n\n        # Plot Energy vs. Nodes for misclassified segments\n        energy_columns = [col for col in X.columns if '_feature_2' in col]\n        energy_values = misclassified_samples[energy_columns].values\n\n        # Create plot\n        plt.figure(figsize=(12, 6))\n        for idx, (sample, label) in enumerate(zip(energy_values, misclassified_labels)):\n            # Choose color based on the actual label\n            color = 'blue' if label == 1 else 'Red'\n            plt.plot(range(len(energy_columns)), sample, label=f'Misclassified Sample {idx+1}', color=color, alpha=0.6)\n\n#     plt.xlabel('Node (Channel)')\n#     plt.ylabel('Energy')\n#     plt.title(f'Energy vs. Nodes for Misclassified Segments (n={n})')\n#     plt.legend(['Anomaly (1)', 'Non-Anomaly (0)'], loc='upper right')\n#     plt.grid(True)\n#     plt.show()\nprint(\"Avg acc = \",sum(acc_list)/len(acc_list))\n# acc_list","metadata":{"execution":{"iopub.status.busy":"2024-11-16T04:32:42.317262Z","iopub.execute_input":"2024-11-16T04:32:42.317671Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## RFE","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import balanced_accuracy_score, classification_report\n\n# Assuming wavelet_features_df_standardized is already defined and loaded with data\n# anomalies = wavelet_features_df_standardized[wavelet_features_df_standardized['anomalies'] == 1]\n# non_anomalies = wavelet_features_df_standardized[wavelet_features_df_standardized['anomalies'] == 0]\n\nanomalies = wavelet_features_df[wavelet_features_df['anomalies'] == 1]\nnon_anomalies = wavelet_features_df[wavelet_features_df['anomalies'] == 0]\n\nacc_list = []\nfor val in range(5,70,5):\n# Sampling a balanced dataset\n    n = min(len(anomalies), len(non_anomalies))\n    balanced_data = pd.concat([\n        anomalies.sample(n=n, random_state=val),\n        non_anomalies.sample(n=n, random_state=val)\n    ])\n\n    # Define features and target variable\n    X = balanced_data.drop(columns=['anomalies'])\n    y = balanced_data['anomalies']\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Initialize the Random Forest Classifier\n    rfc = RandomForestClassifier(random_state=42)\n\n    # Initialize RFE with RandomForestClassifier as the estimator\n    rfe = RFE(estimator=rfc, n_features_to_select=40)  # Adjust n_features_to_select based on desired feature count\n\n    # Fit RFE to training data\n    rfe.fit(X_train, y_train)\n\n    # Transform data to select important features\n    X_train_rfe = rfe.transform(X_train)\n    X_test_rfe = rfe.transform(X_test)\n\n    # Train the model on reduced features\n    rfc.fit(X_train_rfe, y_train)\n\n    # Predict on the test set\n    y_pred = rfc.predict(X_test_rfe)\n\n    # Calculate and print balanced accuracy\n    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n    print(f\"Balanced Accuracy with RFE: {balanced_acc:.4f}\")\n    \n    acc_list.append(balanced_acc)\n\n    # Optional: Classification report\n    print(\"Classification Report with RFE:\\n\", classification_report(y_test, y_pred))\n\nprint(\"average acc = \",sum(acc_list)/len(acc_list))\n","metadata":{"execution":{"iopub.status.busy":"2024-11-13T08:08:45.316197Z","iopub.execute_input":"2024-11-13T08:08:45.316669Z","iopub.status.idle":"2024-11-13T08:08:46.580483Z","shell.execute_reply.started":"2024-11-13T08:08:45.316631Z","shell.execute_reply":"2024-11-13T08:08:46.579202Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# READ the downloaded file 1.e; the dataframe","metadata":{}},{"cell_type":"code","source":"\nwavelet_features_df = pd.read_csv(\"/kaggle/input/wavelet-features-df-of-test-annotation/output.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T11:29:26.243006Z","iopub.execute_input":"2024-11-15T11:29:26.24363Z","iopub.status.idle":"2024-11-15T11:29:27.88149Z","shell.execute_reply.started":"2024-11-15T11:29:26.243594Z","shell.execute_reply":"2024-11-15T11:29:27.880548Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## RFC (partially annotated dataset)","metadata":{}},{"cell_type":"code","source":"# Calculate the number of rows representing the first 20% of the DataFrame\npercent = 0.30\npercent_index = int(percent * len(wavelet_features_df))\nprint(percent_index)\n# Slice the first 20% of the DataFrame\nfirst_percent_df = wavelet_features_df.iloc[:percent_index]\n\n# Count the number of anomalies in the first x% of the DataFrame\nanomaly_count_first_percent = (first_percent_df['anomalies'] == 1).sum()\n\nprint(f\"Number of anomalies in the first {percent*100}% of the DataFrame:\", anomaly_count_first_percent)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:18:03.466194Z","iopub.execute_input":"2024-11-14T16:18:03.46694Z","iopub.status.idle":"2024-11-14T16:18:03.473752Z","shell.execute_reply.started":"2024-11-14T16:18:03.466907Z","shell.execute_reply":"2024-11-14T16:18:03.472887Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import balanced_accuracy_score, classification_report\nfrom sklearn.utils import resample\nimport numpy as np\nimport pandas as pd\naccuracies = []\nfor percent in range(30,90,10):\n    percent /= 100\n    percent_index = int(percent * len(wavelet_features_df))\n    # Splitting the data into 30% for training and 70% for testing\n    \n    train_df = wavelet_features_df.iloc[:percent_index]\n    test_df = wavelet_features_df.iloc[percent_index:]\n    \n    # Separate the minority and majority classes in the training set\n    anomalies = train_df[train_df['anomalies'] == 1]\n    non_anomalies = train_df[train_df['anomalies'] == 0]\n    \n    # Downsampling the majority class to match the minority class size\n    non_anomalies_downsampled = resample(non_anomalies, \n                                         replace=False,  # sample without replacement\n                                         n_samples=len(anomalies),  # match number of anomalies\n                                         random_state=42)  # fixed seed for reproducibility\n    \n    # Create a balanced training dataset\n    balanced_train_df = pd.concat([anomalies, non_anomalies_downsampled])\n    \n    # Separate features and target for training and testing\n    X_train = balanced_train_df.drop(columns=['anomalies'])\n    y_train = balanced_train_df['anomalies']\n    X_test = test_df.drop(columns=['anomalies'])\n    y_test = test_df['anomalies']\n    \n    # Initialize list to store balanced accuracy scores for each seed\n    seeds = np.arange(20)  # 20 different seeds\n    balanced_accuracy_scores = []\n    \n    # Loop over each random seed to train and evaluate\n    for seed in seeds:\n        # Initialize the Random Forest Classifier with the current seed\n        rfc = RandomForestClassifier(random_state=seed)\n    \n        # Train the model on the balanced dataset\n        rfc.fit(X_train, y_train)\n    \n        # Make predictions on the test set (remaining 70% of the data)\n        y_pred = rfc.predict(X_test)\n    \n        # Calculate balanced accuracy and store it\n        balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n        balanced_accuracy_scores.append(balanced_accuracy)\n    \n        # Print classification report for each seed\n        # print(f\"Classification Report for random state={seed}:\\n\", classification_report(y_test, y_pred))\n        # print(\"=\"*50)\n    \n    # Calculate and print the average balanced accuracy over all seeds\n    average_balanced_accuracy = np.mean(balanced_accuracy_scores)\n    print(f\"Average balanced accuracy over 20 seeds with data balancing for split {percent}:\", average_balanced_accuracy)\n    accuracies.append(average_balanced_accuracy)\n    \nprint(max(accuracies))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:18:11.589446Z","iopub.execute_input":"2024-11-14T16:18:11.590297Z","iopub.status.idle":"2024-11-14T16:19:42.857851Z","shell.execute_reply.started":"2024-11-14T16:18:11.590262Z","shell.execute_reply":"2024-11-14T16:19:42.856776Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\n\n# Assuming wavelet_features_df is your input DataFrame\n# Step 1: Normalize the data to improve clustering accuracy\nscaler = StandardScaler()\nnormalized_features = scaler.fit_transform(wavelet_features_df)\n\n# Step 2: Initialize K-means and fit the model\n# Start with 2 clusters (anomalous and non-anomalous)\nkmeans = KMeans(n_clusters=2, random_state=42)\nwavelet_features_df['cluster'] = kmeans.fit_predict(normalized_features)\n\n# Step 3: Calculate Silhouette Score (Optional for evaluation)\nsil_score = silhouette_score(normalized_features, wavelet_features_df['cluster'])\nprint(f'Silhouette Score: {sil_score}')\n\n# Step 4: Classify clusters as \"anomalous\" or \"non-anomalous\"\n# Here, we assume the smaller cluster represents anomalies\n# Find the cluster with the smaller size\ncluster_sizes = wavelet_features_df['cluster'].value_counts()\nanomalous_cluster = cluster_sizes.idxmin()\n\n# Step 5: Assign \"anomalous\" or \"non-anomalous\" labels based on cluster assignment\nwavelet_features_df['anomaly'] = wavelet_features_df['cluster'].apply(\n    lambda x: 'anomalous' if x == anomalous_cluster else 'non-anomalous'\n)\n\n# Step 6: Drop the temporary 'cluster' column if no longer needed\nwavelet_features_df.drop(columns=['cluster'], inplace=True)\n\n# Print the resulting DataFrame with anomaly labels\nprint(wavelet_features_df[['anomaly']].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T15:43:29.993264Z","iopub.execute_input":"2024-11-14T15:43:29.993653Z","iopub.status.idle":"2024-11-14T15:43:32.545616Z","shell.execute_reply.started":"2024-11-14T15:43:29.993619Z","shell.execute_reply":"2024-11-14T15:43:32.544661Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\n# Assuming 'anomalies' is your ground truth column in wavelet_features_df\n# and 'anomaly' is the predicted label from the K-means clustering step.\n\n# Step 1: Map the 'anomalous' and 'non-anomalous' values to binary labels for comparison\n# Ground truth: 'anomalous' should map to 1, 'non-anomalous' to 0\nwavelet_features_df['ground_truth'] = wavelet_features_df['anomalies'].map(lambda x: 1 if x == 'anomalous' else 0)\nwavelet_features_df['predicted'] = wavelet_features_df['anomaly'].map(lambda x: 1 if x == 'anomalous' else 0)\n\n# Step 2: Calculate evaluation metrics\naccuracy = accuracy_score(wavelet_features_df['ground_truth'], wavelet_features_df['predicted'])\nprecision = precision_score(wavelet_features_df['ground_truth'], wavelet_features_df['predicted'])\nrecall = recall_score(wavelet_features_df['ground_truth'], wavelet_features_df['predicted'])\nf1 = f1_score(wavelet_features_df['ground_truth'], wavelet_features_df['predicted'])\nconf_matrix = confusion_matrix(wavelet_features_df['ground_truth'], wavelet_features_df['predicted'])\nbalanced_accuracy = balanced_accuracy_score(wavelet_features_df['ground_truth'], wavelet_features_df['predicted'])\n\n# Print the evaluation results\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1)\ntn, fp, fn, tp = confusion_matrix(wavelet_features_df['ground_truth'], wavelet_features_df['predicted']).ravel()\nspecificity = tn / (tn + fp)\nprint(\"Specificity:\", specificity)\nprint(\"Balanced Accuracy: \",(specificity+recall)/2)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T15:50:50.769476Z","iopub.execute_input":"2024-11-14T15:50:50.770288Z","iopub.status.idle":"2024-11-14T15:50:50.811788Z","shell.execute_reply.started":"2024-11-14T15:50:50.770256Z","shell.execute_reply":"2024-11-14T15:50:50.8109Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wavelet_features_df = pd.read_csv(\"/kaggle/input/wavelet-features-df-of-test-annotation/output.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T04:28:49.962148Z","iopub.execute_input":"2024-11-15T04:28:49.962664Z","iopub.status.idle":"2024-11-15T04:28:51.574711Z","shell.execute_reply.started":"2024-11-15T04:28:49.962624Z","shell.execute_reply":"2024-11-15T04:28:51.573955Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import balanced_accuracy_score, classification_report\nfrom sklearn.utils import resample\nfrom sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix\n\naccuracies = []\nfor percent in range(30,90,10):\n    percent /= 100\n    percent_index = int(percent * len(wavelet_features_df))\n    \n    # Splitting the data \n    train_df = wavelet_features_df.iloc[:percent_index]\n    test_df = wavelet_features_df.iloc[percent_index:]\n    \n    # Separate the minority and majority classes in the training set\n    anomalies = train_df[train_df['anomalies'] == 1]\n    non_anomalies = train_df[train_df['anomalies'] == 0]\n    \n    # Downsampling the majority class to match the minority class size\n    non_anomalies_downsampled = resample(non_anomalies, \n                                         replace=False,  # sample without replacement\n                                         n_samples=len(anomalies),  # match number of anomalies\n                                         random_state=42)  # fixed seed for reproducibility\n    \n    # Create a balanced training dataset\n    balanced_train_df = pd.concat([anomalies, non_anomalies_downsampled])\n    \n    # Separate features and target for training and testing\n    # X_train = balanced_train_df.drop(columns=['anomalies'])\n    # y_train = balanced_train_df['anomalies']\n    # X_test = test_df.drop(columns=['anomalies'])\n    # y_test = test_df['anomalies']\n    \n    # Split data into features (X) and target (y)\n    X = wavelet_features_df.drop(columns=['anomalies'])\n    y = wavelet_features_df['anomalies']\n    \n    # # Split data into training and testing sets\n    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1-percent, random_state=42)\n    \n    # Train an SVM model\n    svm_model = SVC(kernel='rbf', random_state=42)\n    svm_model.fit(X_train, y_train)\n    \n    # Predict on the test set\n    y_pred = svm_model.predict(X_test)\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(y_test, y_pred)\n    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n    \n    # Calculate specificity\n    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n    specificity = tn / (tn + fp)\n    \n    # Print results\n    # print(\"Accuracy:\", accuracy)\n    print(\"Balanced Accuracy:\", balanced_accuracy)\n    # print(\"Specificity:\", specificity)\n    accuracies.append(balanced_accuracy)\n    \nprint(max(accuracies))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T04:38:43.048505Z","iopub.execute_input":"2024-11-15T04:38:43.048993Z","iopub.status.idle":"2024-11-15T04:38:58.32337Z","shell.execute_reply.started":"2024-11-15T04:38:43.048951Z","shell.execute_reply":"2024-11-15T04:38:58.322326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# # from sklearn.linear_model import LassoCV\n# from sklearn.linear_model import Lasso, LassoCV\n# from sklearn.metrics import balanced_accuracy_score\n# from sklearn.model_selection import train_test_split\n\n# # Assuming wavelet_features_df is your input DataFrame with feature columns and 'anomalies' as target\n# # Define features and target variable\n# X = wavelet_features_df_standardized.drop(columns=['anomalies'])\n# y = wavelet_features_df_standardized['anomalies']\n\n# # Initialize a list to store balanced accuracy scores\n# acc_list = []\n# random_states = [0, 10, 20, 30, 40]  # You can add more random states for better averaging\n\n# # Loop over multiple random states for stability\n# for state in random_states:\n#     # Split the data into training and testing sets for each random state\n#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=state)\n    \n#     # Initialize Lasso with cross-validation for optimal alpha\n# #     lasso_cv = LassoCV(cv=5, random_state=state)\n#     lasso_cv = LassoCV(cv=5, random_state=state, max_iter=5000)\n#     lasso_cv.fit(X_train, y_train)\n    \n#     # Use the best alpha found by LassoCV\n#     best_alpha = lasso_cv.alpha_\n#     print(f\"Random State {state} | Optimal alpha: {best_alpha}\")\n\n#     # Fit Lasso model using best alpha\n#     lasso = Lasso(alpha=best_alpha, random_state=state)\n#     lasso.fit(X_train, y_train)\n    \n#     # Predict on the test set\n#     y_pred = (lasso.predict(X_test) > 0.5).astype(int)\n    \n#     # Calculate and store balanced accuracy\n#     balanced_acc = balanced_accuracy_score(y_test, y_pred)\n#     acc_list.append(balanced_acc)\n#     print(f\"Balanced Accuracy for Random State {state}: {balanced_acc:.4f}\")\n\n# # Average the balanced accuracy scores over all random states\n# avg_balanced_accuracy = np.mean(acc_list)\n# print(f\"\\nAverage Balanced Accuracy over random states: {avg_balanced_accuracy:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-08T04:38:10.148146Z","iopub.status.idle":"2024-11-08T04:38:10.148486Z","shell.execute_reply.started":"2024-11-08T04:38:10.148329Z","shell.execute_reply":"2024-11-08T04:38:10.148343Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Wavelet (pywt.WaveletPacket) Incremental approach","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2024-10-30T11:16:41.968071Z","iopub.execute_input":"2024-10-30T11:16:41.968779Z","iopub.status.idle":"2024-10-30T11:16:41.975205Z","shell.execute_reply.started":"2024-10-30T11:16:41.968746Z","shell.execute_reply":"2024-10-30T11:16:41.973969Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize variables\nsegment_length = 1280\nsampling_rate = 256\nbatch_size = 100  # Adjust based on memory constraints\nfinal_balanced_accuracies = []\n\n# Loop through different random states for variability\n# for val in range(5,6 , 5):\nall_samples = []  # Reset to gather new balanced data samples for each iteration\n\n# Loop through data in batches\nfor start in range(0, len(df), batch_size * segment_length):\n    end = start + batch_size * segment_length\n    batch = df.iloc[start:end]\n\n    if len(batch) < segment_length:\n        continue  # Skip if batch is too small\n\n    wavelet_features = []\n    anomaly_labels = []\n\n    # Process each segment in the batch\n    for i in range(0, len(batch), segment_length):\n        segment = batch[['EOG LOC-M2', 'EOG ROC-M1', 'EMG Chin2-Chin1']].iloc[i:i+segment_length]\n        if len(segment) == segment_length:\n            segment_features = []\n            for channel in segment.columns:\n                channel_data = segment[channel].values\n                features = extract_wavelet_features(channel_data)\n                segment_features.extend(features)\n            wavelet_features.append(segment_features)\n\n            # Label the segment as an anomaly if more than 20% of rows are anomalies\n            segment_anomalies = batch['anomalies'].iloc[i:i+segment_length].values\n            if np.sum(segment_anomalies) > 0.5 * segment_length:\n                anomaly_labels.append(1)\n            else:\n                anomaly_labels.append(0)\n\n    # Convert wavelet features to DataFrame\n    wavelet_features_df = pd.DataFrame(wavelet_features)\n    wavelet_features_df['anomalies'] = anomaly_labels\n\n    # Balance the dataset\n    anomalies = wavelet_features_df[wavelet_features_df['anomalies'] == 1]\n    non_anomalies = wavelet_features_df[wavelet_features_df['anomalies'] == 0]\n\n    # Check if there are sufficient samples in both classes\n    min_samples = min(len(anomalies), len(non_anomalies))\n    if min_samples == 0:\n        continue  # Skip this batch if any class has no samples\n\n    # Sample to balance the classes and append to aggregate list\n    anomalies_sample = anomalies.sample(n=min_samples, random_state=val)\n    non_anomalies_sample = non_anomalies.sample(n=min_samples, random_state=val)\n    all_samples.append(pd.concat([anomalies_sample, non_anomalies_sample]))\n\n# Concatenate all batches into a single DataFrame for training\naggregated_data = pd.concat(all_samples)\nX = aggregated_data.drop(columns=['anomalies'])\ny = aggregated_data['anomalies']","metadata":{"execution":{"iopub.status.busy":"2024-10-30T11:29:56.275879Z","iopub.execute_input":"2024-10-30T11:29:56.27622Z","iopub.status.idle":"2024-10-30T11:39:27.287246Z","shell.execute_reply.started":"2024-10-30T11:29:56.276194Z","shell.execute_reply":"2024-10-30T11:39:27.286414Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split the aggregated data into training and testing sets\nfor val in range(5,100,5):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=val)\n\n    # Initialize and train the Random Forest Classifier\n    rfc = RandomForestClassifier(random_state=val)\n    rfc.fit(X_train, y_train)\n    y_pred = rfc.predict(X_test)\n\n    # Calculate balanced accuracy for this iteration\n    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n    final_balanced_accuracies.append(balanced_acc)\n    print(f\"Balanced Accuracy for random state {val}: {balanced_acc:.4f}\")\n\n# Calculate and print the overall average balanced accuracy across iterations\noverall_avg_accuracy = np.mean(final_balanced_accuracies)\nprint(f\"Overall Average Balanced Accuracy: {overall_avg_accuracy:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-30T11:40:01.784805Z","iopub.execute_input":"2024-10-30T11:40:01.785156Z","iopub.status.idle":"2024-10-30T11:40:10.060091Z","shell.execute_reply.started":"2024-10-30T11:40:01.785129Z","shell.execute_reply":"2024-10-30T11:40:10.0589Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X.shape,y.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-30T11:07:34.667028Z","iopub.execute_input":"2024-10-30T11:07:34.667511Z","iopub.status.idle":"2024-10-30T11:07:34.673603Z","shell.execute_reply.started":"2024-10-30T11:07:34.667477Z","shell.execute_reply":"2024-10-30T11:07:34.67258Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## FFT on segment","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy.fft import fft\n\n# Assuming smoothed_n2_df is your DataFrame with all the relevant columns\n# List of columns to process\ncolumns_to_process = ['EOG LOC-M2', 'EOG ROC-M1', 'EMG Chin2-Chin1', 'EEG F3-M2', 'EEG F4-M1', 'EEG C3-M2', \n                      'EEG C4-M1', 'EEG O1-M2', 'EEG O2-M1', 'EEG CZ-O1', 'EMG LLeg-RLeg', 'ECG EKG2-EKG', \n                      'Snore', 'Resp PTAF', 'Resp Airflow', 'Resp Thoracic', 'Resp Abdominal', 'SpO2', 'Rate',  'Capno', 'Resp Rate', 'C-flow', 'Tidal Vol', 'Pressure']\n\n# Parameters\nsegment_size = 1280  # 10 seconds segment (1280 rows)\nanomaly_threshold = 0.10  # 10% anomaly threshold\n\n# Lists to store results\nfeature_rows = []\n\n# Loop through the data and segment it for each column\nfor start_idx in range(0, len(df), segment_size):\n    end_idx = start_idx + segment_size\n    \n    # Ensure segment does not go out of bounds\n    if end_idx > len(df):\n        break\n    \n    # Store features for each column in this segment\n    segment_features = {}\n    \n    # Loop through each column\n    for col in columns_to_process:\n        # Extract the data and annotations for the current column\n        segment = df[col].iloc[start_idx:end_idx].values\n        segment_annotations = df['anomalies'].iloc[start_idx:end_idx].values\n\n        # Check if the segment contains more than 10% anomalies\n        if np.mean(segment_annotations) > anomaly_threshold:\n            segment_label = 1  # Anomalous\n        else:\n            segment_label = 0  # Non-anomalous\n\n        # Apply FFT to the segment\n        fft_segment = fft(segment)\n        \n        # Compute the frequency bins\n        n = len(segment)\n        freqs = np.fft.fftfreq(n)\n\n        # Calculate the amplitude spectrum\n        amplitude_segment = np.abs(fft_segment)\n        # print(amplitude_segment.shape)\n\n        # Extract the dominant frequency (excluding zero frequency)\n        dominant_freq = freqs[np.argmax(amplitude_segment[1:n//2])]\n\n        # Store features: dominant frequency, mean amplitude, and mean value\n        segment_features[f'{col}_dominant_freq'] = dominant_freq\n        segment_features[f'{col}_mean_amplitude'] = np.mean(amplitude_segment)  # Mean of the amplitude\n        segment_features[f'{col}_mean_value'] = np.mean(segment)  # Mean of the original segment\n\n    # Add the label\n    segment_features['label'] = segment_label\n    \n    # Append to feature rows\n    feature_rows.append(segment_features)\n\n# Create a DataFrame from the extracted features\nfeature_df = pd.DataFrame(feature_rows)\n\n# Display the resulting DataFrame\n# print(feature_df)","metadata":{"execution":{"iopub.status.busy":"2024-10-20T13:48:02.964656Z","iopub.execute_input":"2024-10-20T13:48:02.965308Z","iopub.status.idle":"2024-10-20T13:48:33.488324Z","shell.execute_reply.started":"2024-10-20T13:48:02.965277Z","shell.execute_reply":"2024-10-20T13:48:33.487527Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(feature_df[feature_df['label']==1],\"   len  =.  \",len(feature_df[feature_df['label']==1]))\nfeature_df.columns","metadata":{"execution":{"iopub.status.busy":"2024-10-19T14:29:52.584243Z","iopub.execute_input":"2024-10-19T14:29:52.585037Z","iopub.status.idle":"2024-10-19T14:29:52.591969Z","shell.execute_reply.started":"2024-10-19T14:29:52.584997Z","shell.execute_reply":"2024-10-19T14:29:52.591081Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, balanced_accuracy_score\n# Separate features and labels\n# X = feature_df[['dominant_freq', 'mean_amplitude', 'mean_spo2']]\nX = feature_df[['EOG LOC-M2_dominant_freq', 'EOG LOC-M2_mean_amplitude',\n       'EOG LOC-M2_mean_value', 'EOG ROC-M1_dominant_freq',\n       'EOG ROC-M1_mean_amplitude', 'EOG ROC-M1_mean_value',\n       'EMG Chin2-Chin1_dominant_freq', 'EMG Chin2-Chin1_mean_amplitude',\n       'EMG Chin2-Chin1_mean_value', 'EEG F3-M2_dominant_freq',\n       'EEG F3-M2_mean_amplitude', 'EEG F3-M2_mean_value',\n       'EEG F4-M1_dominant_freq', 'EEG F4-M1_mean_amplitude',\n       'EEG F4-M1_mean_value', 'EEG C3-M2_dominant_freq',\n       'EEG C3-M2_mean_amplitude', 'EEG C3-M2_mean_value',\n       'EEG C4-M1_dominant_freq', 'EEG C4-M1_mean_amplitude',\n       'EEG C4-M1_mean_value', 'EEG O1-M2_dominant_freq',\n       'EEG O1-M2_mean_amplitude', 'EEG O1-M2_mean_value',\n       'EEG O2-M1_dominant_freq', 'EEG O2-M1_mean_amplitude',\n       'EEG O2-M1_mean_value', 'EEG CZ-O1_dominant_freq',\n       'EEG CZ-O1_mean_amplitude', 'EEG CZ-O1_mean_value',\n       'EMG LLeg-RLeg_dominant_freq', 'EMG LLeg-RLeg_mean_amplitude',\n       'EMG LLeg-RLeg_mean_value', 'ECG EKG2-EKG_dominant_freq',\n       'ECG EKG2-EKG_mean_amplitude', 'ECG EKG2-EKG_mean_value',\n       'Snore_dominant_freq', 'Snore_mean_amplitude', 'Snore_mean_value',\n       'Resp PTAF_dominant_freq', 'Resp PTAF_mean_amplitude',\n       'Resp PTAF_mean_value', 'Resp Airflow_dominant_freq',\n       'Resp Airflow_mean_amplitude', 'Resp Airflow_mean_value',\n       'Resp Thoracic_dominant_freq', 'Resp Thoracic_mean_amplitude',\n       'Resp Thoracic_mean_value', 'Resp Abdominal_dominant_freq',\n       'Resp Abdominal_mean_amplitude', 'Resp Abdominal_mean_value',\n       'SpO2_dominant_freq', 'SpO2_mean_amplitude', 'SpO2_mean_value',\n       'Rate_dominant_freq', 'Rate_mean_amplitude', 'Rate_mean_value',\n       'Capno_dominant_freq', 'Capno_mean_amplitude', 'Capno_mean_value',\n       'Resp Rate_dominant_freq', 'Resp Rate_mean_amplitude',\n       'Resp Rate_mean_value', 'C-flow_dominant_freq', 'C-flow_mean_amplitude',\n       'C-flow_mean_value', 'Tidal Vol_dominant_freq',\n       'Tidal Vol_mean_amplitude', 'Tidal Vol_mean_value',\n       'Pressure_dominant_freq', 'Pressure_mean_amplitude',\n       'Pressure_mean_value']]\ny = feature_df['label']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n\n# Initialize the Random Forest Classifier\nrfc = RandomForestClassifier(random_state=42)\n\n# Train the model\nrfc.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = rfc.predict(X_test)\n\n# Evaluate the model\nprint(classification_report(y_test, y_pred))\nbalanced_acc = balanced_accuracy_score(y_test, y_pred)\nprint(f\"Balanced Accuracy: {balanced_acc:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-19T14:31:04.248875Z","iopub.execute_input":"2024-10-19T14:31:04.249238Z","iopub.status.idle":"2024-10-19T14:31:06.828112Z","shell.execute_reply.started":"2024-10-19T14:31:04.249209Z","shell.execute_reply":"2024-10-19T14:31:06.827207Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-18T13:21:25.526953Z","iopub.execute_input":"2024-10-18T13:21:25.527343Z","iopub.status.idle":"2024-10-18T13:21:25.553868Z","shell.execute_reply.started":"2024-10-18T13:21:25.527306Z","shell.execute_reply":"2024-10-18T13:21:25.55295Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy.fft import fft\n\n# Assuming your dataset is in a pandas DataFrame called 'df' and it contains the signal columns.\n\n# Define the sampling frequency and segment size\nsampling_frequency = 256  # Hz\nsegment_size = 1280  # 5 seconds of data (1280 rows)\n\ndef compute_fft(segment):\n    # Ensure the segment is a numpy array\n    segment = np.array(segment)\n    \n    # Check if there are any missing values\n    if np.isnan(segment).any():\n        segment = np.nan_to_num(segment)  # Replace NaNs with 0\n    \n    # Apply FFT\n    fft_result = fft(segment)\n    \n    # Calculate power spectrum\n    power_spectrum = np.abs(fft_result)**2\n    \n    return power_spectrum[:len(power_spectrum)//2]  # Only take positive frequencies\n\n# Select only numeric columns for signal data\nsignal_columns = ['SpO2']  # Modify this based on your actual signal channels\n\n# Initialize an empty list to store the amplitude data (power spectrum)\namplitude_list = []\n\n# Loop through each channel and segment, and apply FFT\nfor channel in signal_columns:\n    channel_amplitudes = []  # List to hold amplitude data for this channel\n    for start in range(0, len(df), segment_size):\n        segment = df[channel].iloc[start:start + segment_size]\n        if len(segment) == segment_size:  # Ensure the segment has the correct size\n            power_spectrum = compute_fft(segment)\n            channel_amplitudes.append(power_spectrum)\n    \n    amplitude_list.append(channel_amplitudes)\n\n# Convert the list to a numpy array\n# Shape will be (num_channels, num_segments, segment_size/2) since we take only positive frequencies\namplitude_array = np.array(amplitude_list)\n\n# Transpose the array to have shape (num_segments, segment_size/2, num_channels)\namplitude_array = np.transpose(amplitude_array, (1, 2, 0))\n\n# Check the shape of the amplitude array\nprint(\"Amplitude array shape:\", amplitude_array.shape)\n\n# Now you can use amplitude_array to train the autoencoder\n# autoencoder.fit(amplitude_array, amplitude_array, epochs=50, batch_size=32, validation_split=0.2)\n\n\n\n# import numpy as np\n# import pandas as pd\n# from scipy.fft import fft\n\n# # Assuming your dataset is in a pandas DataFrame called 'df' and it contains the signal columns.\n\n# # Define the sampling frequency and segment size\n# sampling_frequency = 256  # Hz\n# segment_size = 1280  # 5 seconds of data (1280 rows)\n\n# def compute_fft(segment):\n#     # Ensure the segment is a numpy array\n#     segment = np.array(segment)\n    \n#     # Debugging information to see the input segment\n#     print(\"Input segment length: \", len(segment))\n#     # print(\"First 5 values of segment: \", segment[:5])\n    \n#     # Check if there are any missing values\n#     if np.isnan(segment).any():\n#         print(\"Warning: Segment contains NaN values. Filling NaNs with 0.\")\n#         segment = np.nan_to_num(segment)  # Replace NaNs with 0\n    \n#     # Apply FFT\n#     print(\"Applying FFT...\")\n#     fft_result = fft(segment)\n#     print(\"FFT result shape: \", fft_result.shape)\n    \n#     # Calculate power spectrum\n#     power_spectrum = np.abs(fft_result)**2\n    \n#     # Calculate frequencies\n#     frequencies = np.fft.fftfreq(len(segment), d=1/sampling_frequency)\n    \n#     # Debugging information to see the output of FFT and frequencies\n#     print(\"First 5 FFT values: \", fft_result[:5])\n#     print(\"First 5 frequencies: \", frequencies[:5])\n    \n#     return frequencies, power_spectrum\n\n\n# # Select only numeric columns for signal data\n# signal_columns = df.select_dtypes(include=[np.number]).columns\n# print(\"signal_columns = \",signal_columns)\n# signal_columns = ['SpO2']\n\n# # Apply FFT to each channel in the dataset in segments of 1280 rows\n# fft_features = []\n\n# # Loop through each channel (assuming channels are in columns)\n# for channel in signal_columns:\n#     for start in range(0, len(df), segment_size):\n#         segment = df[channel].iloc[start:start + segment_size]\n#         if len(segment) == segment_size:  # Ensure the segment has the correct size\n#             # print(\"segment = \",segment)\n#             print(\"---------\\n\")\n#             freqs, power = compute_fft(segment)\n#             fft_features.append({\n#                 'channel': channel,\n#                 'frequencies': freqs[:len(freqs)//2],  # Only take positive frequencies\n#                 'power_spectrum': power[:len(power)//2]  # Positive frequency components\n#             })\n\n# # Convert FFT features into a suitable format for further analysis\n# fft_df = pd.DataFrame(fft_features)\n\n# # Display the FFT DataFrame\n# print(fft_df)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T13:27:57.563744Z","iopub.execute_input":"2024-10-20T13:27:57.564459Z","iopub.status.idle":"2024-10-20T13:27:58.463566Z","shell.execute_reply.started":"2024-10-20T13:27:57.564427Z","shell.execute_reply":"2024-10-20T13:27:58.462405Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df.columns\namplitude_array[:1].shape","metadata":{"execution":{"iopub.status.busy":"2024-10-20T13:29:12.62602Z","iopub.execute_input":"2024-10-20T13:29:12.627018Z","iopub.status.idle":"2024-10-20T13:29:12.634767Z","shell.execute_reply.started":"2024-10-20T13:29:12.626978Z","shell.execute_reply":"2024-10-20T13:29:12.633726Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy.fft import fft\n\n# Sample DataFrame with 25 columns (replace with actual DataFrame)\n# df = pd.DataFrame(...)\n\n# Assuming 'df' contains your data with 25 columns\ncolumns = ['EOG LOC-M2', 'EOG ROC-M1', 'EMG Chin2-Chin1', 'EEG F3-M2', 'EEG F4-M1',\n           'EEG C3-M2', 'EEG C4-M1', 'EEG O1-M2', 'EEG O2-M1', 'EEG CZ-O1',\n           'EMG LLeg-RLeg', 'ECG EKG2-EKG', 'Snore', 'Resp PTAF', 'Resp Airflow',\n           'Resp Thoracic', 'Resp Abdominal', 'SpO2', 'Rate', 'Capno',\n           'Resp Rate', 'C-flow', 'Tidal Vol', 'Pressure']\n\n# Define the sampling frequency and segment size\nsampling_frequency = 256  # Hz\nsegment_size = 1280  # 5 seconds of data (1280 rows)\n\ndef compute_fft(segment):\n    # Ensure the segment is a numpy array\n    segment = np.array(segment)\n    \n    # Check if there are any missing values\n    if np.isnan(segment).any():\n        segment = np.nan_to_num(segment)  # Replace NaNs with 0\n    \n    # Apply FFT\n    fft_result = fft(segment)\n\n    print(\"fft_result = \",fft_result.shape)\n    # Calculate power spectrum\n    power_spectrum = np.abs(fft_result)**2\n    \n    return power_spectrum[:len(power_spectrum)//2]  # Only take positive frequencies\n\n# Define the segment size (e.g., 1280 rows for 5 seconds at 256Hz)\nsegment_size = 1280\n\n# Initialize an empty DataFrame to store FFT results\nfft_df = pd.DataFrame()\n\n# Loop through the DataFrame and apply FFT to each segment of each column\nfor i in range(0, 1280, segment_size): #len(df),\n    segment_fft_row = []  # Temporary list to hold FFT values for this segment\n    \n    # Extract a segment (1280 rows) for each column\n    for col in columns:\n        if col in df.columns:  # Check if the column exists in the DataFrame\n            # print(\"col = \",col)\n            segment = df[col].iloc[i:i+segment_size]  # Get 1280 rows (5 seconds)\n            # print(\"len(segment) = \",len(segment))\n            if len(segment) == segment_size:  # Ensure full segment size\n                # print(\"IN IF\")\n                fft_values = compute_fft(segment)  # Apply FFT\n                # print(\"fft_values = \",fft_values)\n                segment_fft_row.extend(fft_values)  # Flatten the result and append\n        else:\n            print(f\"Column {col} not found in the DataFrame.\")\n\n    fft_values_df = pd.DataFrame(segment_fft_row)\n    fft_df = pd.concat([fft_df, fft_values_df], axis=1)\n    # If segment FFT is computed, append it as a row in the DataFrame\n    # print(segment_fft_row)\n\n    # if segment_fft_row:\n    #     # Add the new row to fft_df DataFrame\n    #     fft_df = pd.concat([fft_df, segment_fft_row], axis=1)\n    #     # fft_df = fft_df.append(pd.Series(segment_fft_row), ignore_index=True)\n\n# Assign column names to the DataFrame based on the number of FFT values\n# fft_df.columns = [f'fft_{col}_{i}' for col in columns for i in range(int(segment_size / 2) + 1)]\n# fft_df.columns = [f'fft_{col}' for col in columns]\n\nprint(fft_df.head())  # Display the first few rows of FFT results\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T14:24:33.96134Z","iopub.execute_input":"2024-10-20T14:24:33.961702Z","iopub.status.idle":"2024-10-20T14:24:33.990663Z","shell.execute_reply.started":"2024-10-20T14:24:33.961671Z","shell.execute_reply":"2024-10-20T14:24:33.989622Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense, Dropout\nfrom keras.models import Model\n\n# Parameters\ntimesteps = 1280  # Segment size (number of time steps in each segment)\nfeatures = 5  # Number of channels\n\n# Input layer with the shape (timesteps, features)\ninput_data = Input(shape=(timesteps, features))\n\n# Encoder\nencoded = LSTM(64, activation='relu', return_sequences=True)(input_data)\nencoded = Dropout(0.2)(encoded)\nencoded = LSTM(32, activation='relu', return_sequences=True)(encoded)\nencoded = Dropout(0.2)(encoded)\nencoded = LSTM(16, activation='relu', return_sequences=False)(encoded)\nencoded = Dropout(0.2)(encoded)\n\n# Repeat vector to match decoder's input shape\ndecoded = RepeatVector(timesteps)(encoded)\n\n# Decoder\ndecoded = LSTM(16, activation='relu', return_sequences=True)(decoded)\ndecoded = Dropout(0.2)(decoded)\ndecoded = LSTM(32, activation='relu', return_sequences=True)(decoded)\ndecoded = Dropout(0.2)(decoded)\ndecoded = LSTM(64, activation='relu', return_sequences=True)(decoded)\ndecoded = Dropout(0.2)(decoded)\n\n# Output layer - reconstructs the signal for each timestep and each channel\ndecoded = TimeDistributed(Dense(features, activation='linear'))(decoded)\n\n# Define the autoencoder model\nautoencoder = Model(inputs=input_data, outputs=decoded)\n\n# Compile the model\nautoencoder.compile(optimizer='adam', loss='mse')\n\n# Display the model architecture\nautoencoder.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T14:06:30.783511Z","iopub.execute_input":"2024-10-18T14:06:30.78389Z","iopub.status.idle":"2024-10-18T14:06:30.932689Z","shell.execute_reply.started":"2024-10-18T14:06:30.783859Z","shell.execute_reply":"2024-10-18T14:06:30.931816Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming amplitude_array contains your data of shape (1000, 1280, 5)\n# You don't need to reshape it since it's already in the correct shape\n# autoencoder.fit(fft_df, fft_df, epochs=50, batch_size=32, validation_split=0.2)\n# Assuming amplitude_array contains your data of shape (1000, 1280, 5)\n# You don't need to reshape it since it's already in the correct shape\nautoencoder.fit(amplitude_array, amplitude_array, epochs=50, batch_size=32, validation_split=0.2)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T14:07:23.040505Z","iopub.execute_input":"2024-10-18T14:07:23.040867Z","iopub.status.idle":"2024-10-18T14:07:23.25035Z","shell.execute_reply.started":"2024-10-18T14:07:23.040838Z","shell.execute_reply":"2024-10-18T14:07:23.248929Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict reconstructed output\nreconstructed = autoencoder.predict(amplitude_array)\n\n# Calculate reconstruction error (MSE)\nreconstruction_error = np.mean(np.power(amplitude_array - reconstructed, 2), axis=(1, 2))  # Error per row","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Pre-processing","metadata":{}},{"cell_type":"markdown","source":"## Smoothing each sleep stage df","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport scipy.signal as signal\nimport matplotlib.pyplot as plt\n\n# Example settings\nfs = 256  # Sampling frequency in Hz\nepoch_duration = 30  # Each EEG epoch is 30 seconds long\nnyquist = 0.5 * fs  # Nyquist frequency\n\n# Frequencies to eliminate (60 Hz and 120 Hz)\nfreqs_to_eliminate = [60, 120]\n\n# Design a 3rd-order Butterworth bandstop filter\ndef butter_bandstop(lowcut, highcut, fs, order=3):\n    low = lowcut / nyquist\n    high = highcut / nyquist\n    b, a = signal.butter(order, [low, high], btype='bandstop')\n    return b, a\n\n# Apply the filter to the EEG signal\ndef apply_bandstop_filter(eeg_signal, fs, freqs_to_eliminate, order=3):\n    filtered_signal = eeg_signal.copy()  # Copy to avoid modifying the original\n    for freq in freqs_to_eliminate:\n        # Create bandstop filters around the target frequencies\n        lowcut = freq - 1  # 1 Hz below the target frequency\n        highcut = freq + 1  # 1 Hz above the target frequency\n        b, a = butter_bandstop(lowcut, highcut, fs, order)\n        # Apply filter to the signal\n        filtered_signal = signal.filtfilt(b, a, filtered_signal)\n    return filtered_signal\n\n# Assuming df is your DataFrame with the 7 EEG columns\neeg_columns = ['EEG F3-M2', 'EEG F4-M1', 'EEG C3-M2', 'EEG C4-M1', 'EEG O1-M2', 'EEG O2-M1', 'EEG CZ-O1']\n\n# Apply the bandstop filter to each EEG column and store the filtered signals\nfiltered_signals = {}\n\n# Apply the bandstop filter and update the EEG columns in the DataFrame\ndef update_eeg_columns_with_filtered_signals(df, fs, freqs_to_eliminate, eeg_columns, order=3):\n#     filtered_signals = {}\n    for column in eeg_columns:\n        eeg_signal = df[column].values  # Extract EEG signal from the DataFrame\n        filtered_signal = apply_bandstop_filter(eeg_signal, fs, freqs_to_eliminate, order)\n        df[column] = filtered_signal  # Update the DataFrame with the filtered signal\n    return df\n\nw_df = update_eeg_columns_with_filtered_signals(w_df, fs, freqs_to_eliminate, eeg_columns)\nn1_df = update_eeg_columns_with_filtered_signals(n1_df, fs, freqs_to_eliminate, eeg_columns)\nn2_df = update_eeg_columns_with_filtered_signals(n2_df, fs, freqs_to_eliminate, eeg_columns)\nn3_df = update_eeg_columns_with_filtered_signals(n3_df, fs, freqs_to_eliminate, eeg_columns)\nr_df = update_eeg_columns_with_filtered_signals(r_df, fs, freqs_to_eliminate, eeg_columns)\n\n# Plot original vs filtered signal for each column\n# plt.figure(figsize=(14, 12))\n# for i, column in enumerate(eeg_columns):\n#     plt.subplot(len(eeg_columns), 2, 2*i + 1)\n#     plt.plot(df.index, df[column])\n#     plt.title(f'Original EEG Signal - {column}')\n    \n#     plt.subplot(len(eeg_columns), 2, 2*i + 2)\n#     plt.plot(df.index, filtered_signals[column])\n#     plt.title(f'Filtered EEG Signal - {column}')\n    \n# plt.tight_layout()\n# plt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-16T02:38:32.237014Z","iopub.execute_input":"2024-11-16T02:38:32.237421Z","iopub.status.idle":"2024-11-16T02:38:37.149074Z","shell.execute_reply.started":"2024-11-16T02:38:32.237389Z","shell.execute_reply":"2024-11-16T02:38:37.147471Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from scipy.signal import savgol_filter\nprint(df.columns)\n# Define the common window size\n# List of columns to smooth (excluding 'anomalies')\ncolumns_to_smooth = n3_df.columns.difference(['anomalies'])\n\n# Apply the moving average filter to each column except 'anomalies'\nsmoothed_w_df = w_df.copy()  # Create a copy to avoid modifying the original DataFrame\nsmoothed_n1_df = n1_df.copy()  # Create a copy to avoid modifying the original DataFrame\nsmoothed_n2_df = n2_df.copy()  # Create a copy to avoid modifying the original DataFrame\nsmoothed_n3_df = n3_df.copy()  # Create a copy to avoid modifying the original DataFrame\nsmoothed_r_df = r_df.copy()  # Create a copy to avoid modifying the original DataFrame\n# Create a copy to avoid modifying the original DataFrame\ncommon_window_size = 512  # Approximately 2 seconds of data\n\nfor col in columns_to_smooth:\n    smoothed_w_df[col] = w_df[col].rolling(window=common_window_size, min_periods=1).mean()\n    smoothed_n1_df[col] = n1_df[col].rolling(window=common_window_size, min_periods=1).mean()\n    smoothed_n2_df[col] = n2_df[col].rolling(window=common_window_size, min_periods=1).mean()\n    smoothed_n3_df[col] = n3_df[col].rolling(window=common_window_size, min_periods=1).mean()\n    smoothed_r_df[col] = r_df[col].rolling(window=common_window_size, min_periods=1).mean()\n    \nprint(smoothed_w_df.head())  # Check the result\nprint(smoothed_n1_df.head())  # Check the result\nprint(smoothed_n2_df.head())  # Check the result\nprint(smoothed_n3_df.head())  # Check the result\nprint(smoothed_r_df.head())    # Create a copy to avoid modifying the original DataFrame","metadata":{"execution":{"iopub.status.busy":"2024-11-16T02:38:46.469441Z","iopub.execute_input":"2024-11-16T02:38:46.470506Z","iopub.status.idle":"2024-11-16T02:38:51.70245Z","shell.execute_reply.started":"2024-11-16T02:38:46.470473Z","shell.execute_reply":"2024-11-16T02:38:51.701342Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"smoothed_w_df.columns,len(smoothed_w_df)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T02:38:51.704506Z","iopub.execute_input":"2024-11-16T02:38:51.704917Z","iopub.status.idle":"2024-11-16T02:38:51.712082Z","shell.execute_reply.started":"2024-11-16T02:38:51.70488Z","shell.execute_reply":"2024-11-16T02:38:51.711044Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(w_df),len(n1_df),len(n2_df),len(n3_df),len(r_df)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T02:38:57.218738Z","iopub.execute_input":"2024-11-16T02:38:57.219088Z","iopub.status.idle":"2024-11-16T02:38:57.225896Z","shell.execute_reply.started":"2024-11-16T02:38:57.219059Z","shell.execute_reply":"2024-11-16T02:38:57.224849Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(smoothed_w_df),len(smoothed_n1_df),len(smoothed_n2_df),len(smoothed_n3_df),len(smoothed_r_df)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T02:39:00.647636Z","iopub.execute_input":"2024-11-16T02:39:00.648514Z","iopub.status.idle":"2024-11-16T02:39:00.654285Z","shell.execute_reply.started":"2024-11-16T02:39:00.648483Z","shell.execute_reply":"2024-11-16T02:39:00.653485Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Standardizing/scaling the per sleep stage df","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\nimport pandas as pd\n\n# Initialize the RobustScaler\nscaler = RobustScaler()\n\n# List of DataFrames to scale\n# dfs_to_scale = [smoothed_w_df, smoothed_n1_df,smoothed_n3_df, smoothed_r_df]\ndfs_to_scale = [smoothed_w_df, smoothed_n1_df, smoothed_n2_df, smoothed_n3_df, smoothed_r_df]\n\n# Perform Robust Scaling on each DataFrame\nscaled_dfs = []\nfor df in dfs_to_scale:\n    # Apply scaling (excluding 'anomalies' column, if it's in the DataFrame)\n    if 'anomalies' in df.columns:\n        features = df.drop(columns=['anomalies'])\n        scaled_features = scaler.fit_transform(features)\n        scaled_df = pd.DataFrame(scaled_features, columns=features.columns)\n        scaled_df['anomalies'] = df['anomalies'].values  # Reattach 'anomalies' column\n    else:\n        scaled_features = scaler.fit_transform(df)\n        scaled_df = pd.DataFrame(scaled_features, columns=df.columns)\n\n    scaled_dfs.append(scaled_df)\n\n# Assign scaled DataFrames to variables\n# scaled_w_df, scaled_n1_df, scaled_n3_df, scaled_r_df = scaled_dfs\nscaled_w_df, scaled_n1_df, scaled_n2_df, scaled_n3_df, scaled_r_df = scaled_dfs\n\n# Print the heads of the scaled DataFrames\nprint(scaled_w_df.head())\nprint(scaled_n1_df.head())\nprint(scaled_n2_df.head())\nprint(scaled_n3_df.head())\nprint(scaled_r_df.head())\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-16T03:27:27.74879Z","iopub.execute_input":"2024-11-16T03:27:27.749204Z","iopub.status.idle":"2024-11-16T03:27:32.631847Z","shell.execute_reply.started":"2024-11-16T03:27:27.749172Z","shell.execute_reply":"2024-11-16T03:27:32.630813Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaled_r_df.columns,len(scaled_r_df.columns),len(scaled_n2_df)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T03:27:32.633435Z","iopub.execute_input":"2024-11-16T03:27:32.633764Z","iopub.status.idle":"2024-11-16T03:27:32.640684Z","shell.execute_reply.started":"2024-11-16T03:27:32.633736Z","shell.execute_reply":"2024-11-16T03:27:32.639647Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# scaled_w_df=scaled_w_df.drop(columns=['Patient Event','anomalies'])\n# scaled_n1_df=scaled_n1_df.drop(columns=['Patient Event','anomalies'])\n# scaled_n2_df=scaled_n2_df.drop(columns=['Patient Event','anomalies'])\n# scaled_n3_df=scaled_n3_df.drop(columns=['Patient Event','anomalies'])\n# scaled_r_df=scaled_r_df.drop(columns=['Patient Event','anomalies'])\n\n# # scaled_w_df.drop('Patient Event')\n# scaled_n3_df=scaled_n3_df.drop(columns=['anomalies'])\n# scaled_r_df=scaled_r_df.drop(columns=['anomalies'])\n# scaled_w_df=scaled_w_df.drop(columns=['anomalies'])\n# scaled_n1_df=scaled_n1_df.drop(columns=['anomalies'])\n# scaled_n2_df=scaled_n2_df.drop(columns=['anomalies'])\n\n# scaled_w_df=scaled_w_df.drop(columns=['EtCO2','anomalies'])\n# scaled_n1_df=scaled_n1_df.drop(columns=['EtCO2','anomalies'])\n# scaled_n2_df=scaled_n2_df.drop(columns=['EtCO2','anomalies'])\n# scaled_n3_df=scaled_n3_df.drop(columns=['EtCO2','anomalies'])\n# scaled_r_df=scaled_r_df.drop(columns=['EtCO2','anomalies'])\n\nscaled_w_df=scaled_w_df.drop(columns=['Patient Event','EtCO2','anomalies'])\nscaled_n1_df=scaled_n1_df.drop(columns=['Patient Event','EtCO2','anomalies'])\nscaled_n2_df=scaled_n2_df.drop(columns=['Patient Event','EtCO2','anomalies'])\nscaled_n3_df=scaled_n3_df.drop(columns=['Patient Event','EtCO2','anomalies'])\nscaled_r_df=scaled_r_df.drop(columns=['Patient Event','EtCO2','anomalies'])","metadata":{"execution":{"iopub.status.busy":"2024-11-16T02:39:30.839365Z","iopub.execute_input":"2024-11-16T02:39:30.840292Z","iopub.status.idle":"2024-11-16T02:39:31.223116Z","shell.execute_reply.started":"2024-11-16T02:39:30.840257Z","shell.execute_reply":"2024-11-16T02:39:31.222137Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Valence Inversion Factor","metadata":{}},{"cell_type":"code","source":"# Assuming 'df' is your DataFrame\nimport pandas as pd\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Calculate VIF for each feature in the DataFrame 'df'\nvif_data = pd.DataFrame()\nvif_data[\"Feature\"] = scaled_r_df.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(scaled_r_df.values, i) for i in range(len(scaled_r_df.columns))]\n\nprint(vif_data)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T05:28:11.652985Z","iopub.execute_input":"2024-10-06T05:28:11.653874Z","iopub.status.idle":"2024-10-06T05:29:41.639059Z","shell.execute_reply.started":"2024-10-06T05:28:11.653837Z","shell.execute_reply":"2024-10-06T05:29:41.635318Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Pair-wise correlation","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming 'df' is your DataFrame with features\ncorrelation_matrix = scaled_r_df.corr()\n\n# Display the correlation matrix\nprint(correlation_matrix)\n\n# Optional: To visualize the correlation matrix using a heatmap\nplt.figure(figsize=(12, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\nplt.title('Pairwise Correlation Matrix')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-11T08:17:02.46174Z","iopub.execute_input":"2024-10-11T08:17:02.462111Z","iopub.status.idle":"2024-10-11T08:17:06.715392Z","shell.execute_reply.started":"2024-10-11T08:17:02.462083Z","shell.execute_reply":"2024-10-11T08:17:06.714481Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Condition number","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Assuming 'df' is your DataFrame with features\nX = scaled_r_df.values  # Convert the DataFrame to a NumPy array\n\n# Calculate the condition number using the singular values\ncondition_number = np.linalg.cond(X)\n\nprint(f\"Condition Number: {condition_number}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T05:13:20.525712Z","iopub.execute_input":"2024-10-13T05:13:20.526123Z","iopub.status.idle":"2024-10-13T05:13:21.069873Z","shell.execute_reply.started":"2024-10-13T05:13:20.526092Z","shell.execute_reply":"2024-10-13T05:13:21.068862Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## PCA","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# Function to perform PCA and plot the explained variance ratio (elbow plot)\ndef plot_pca_explained_variance(df):\n    \"\"\"\n    This function performs PCA on the DataFrame and plots the cumulative explained variance \n    to help determine the optimal number of components using the elbow method.\n    \n    :param df: Input DataFrame.\n    :return: PCA model (sklearn PCA object) for further use.\n    \"\"\"\n    # Apply PCA with as many components as there are features\n    pca = PCA(n_components=min(df.shape[0], df.shape[1]))\n    pca.fit(df)\n    \n    # Calculate the cumulative explained variance\n    cumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n    \n    # Plot the elbow graph\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', linestyle='--')\n    plt.title('Explained Variance by Number of PCA Components')\n    plt.xlabel('Number of Components')\n    plt.ylabel('Cumulative Explained Variance')\n    plt.axhline(y=0.95, color='r', linestyle='--', label='95% Explained Variance')  # Optional 95% line\n    plt.legend()\n    plt.grid()\n    plt.show()\n    \n    return pca\n\n# Apply to your DataFrames (example: scaled_w_df)\n# pca_model_w = plot_pca_explained_variance(scaled_w_df)\n# pca_model_n1 = plot_pca_explained_variance(scaled_n1_df)\n# pca_model_n2 = plot_pca_explained_variance(scaled_n2_df)\n# pca_model_n3 = plot_pca_explained_variance(scaled_n3_df)\npca_model_r = plot_pca_explained_variance(scaled_r_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T05:13:24.528447Z","iopub.execute_input":"2024-10-13T05:13:24.528819Z","iopub.status.idle":"2024-10-13T05:13:26.068111Z","shell.execute_reply.started":"2024-10-13T05:13:24.528789Z","shell.execute_reply":"2024-10-13T05:13:26.067246Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming you've used sklearn's PCA\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\n\n# Let's assume 'data' is your dataset and 'pca' is the fitted PCA model\n# Perform PCA on your dataset\npca = PCA()\npca.fit(scaled_r_df)\n\n# Get the explained variance ratios\nexplained_variance_ratios = pca.explained_variance_ratio_\n\n# Get the cumulative explained variance\ncumulative_variance = np.cumsum(explained_variance_ratios)\n\n# Find out how many components are needed to explain 96% variance\nn_components_96 = np.argmax(cumulative_variance >= 0.96) + 1\nprint(f\"Number of components to reach 96% variance: {n_components_96}\")\n\n# Get the PCA components (loadings)\nloadings = pca.components_\n\n# Create a DataFrame with the features and their contributions to each principal component\n# Assuming 'feature_names' is the list of original feature names in your dataset\nloadings_df = pd.DataFrame(loadings[:n_components_96], columns=scaled_r_df.columns)\n\n# Print the contributions of features for the selected components\nprint(loadings_df)\n\n# Calculate the magnitude of the loadings\nloadings_df['magnitude'] = loadings_df.abs().max(axis=1)\n\n# Sort the loadings dataframe by magnitude in descending order\nsorted_loadings_df = loadings_df.sort_values(by='magnitude', ascending=False)\n\n# Drop the magnitude column if you want to remove it from the output\nsorted_loadings_df = sorted_loadings_df.drop('magnitude', axis=1)\n\n# Print the sorted loadings dataframe\nfor i in sorted_loadings_df:\n    print(i)\n# print(sorted_loadings_df)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T05:13:49.08091Z","iopub.execute_input":"2024-10-13T05:13:49.081516Z","iopub.status.idle":"2024-10-13T05:13:50.109108Z","shell.execute_reply.started":"2024-10-13T05:13:49.081486Z","shell.execute_reply":"2024-10-13T05:13:50.108119Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Set pandas to display all columns\npd.set_option('display.max_columns', None)\n\n# Calculate the magnitude of the loadings\nloadings_df['magnitude'] = loadings_df.abs().max(axis=1)\n\n# Sort the loadings dataframe by magnitude in descending order\nsorted_loadings_df = loadings_df.sort_values(by='magnitude', ascending=False)\n\n# Drop the magnitude column if you don't want to display it\nsorted_loadings_df = sorted_loadings_df.drop('magnitude', axis=1)\n\n# Print the sorted loadings dataframe\nprint(sorted_loadings_df)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T05:13:59.399173Z","iopub.execute_input":"2024-10-13T05:13:59.399865Z","iopub.status.idle":"2024-10-13T05:13:59.423532Z","shell.execute_reply.started":"2024-10-13T05:13:59.399833Z","shell.execute_reply":"2024-10-13T05:13:59.422573Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sorted_loadings_df.columns,sorted_loadings_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-13T05:14:48.648606Z","iopub.execute_input":"2024-10-13T05:14:48.649006Z","iopub.status.idle":"2024-10-13T05:14:48.655825Z","shell.execute_reply.started":"2024-10-13T05:14:48.648976Z","shell.execute_reply":"2024-10-13T05:14:48.654759Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Graph per sleep stage for anomaly n non-anomaly","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Example column to plot\n# column_to_plot = 'Resp Rate'  # Replace with your actual column name\ncolumn_to_plot = 'Resp Rate'  # Replace with your actual column name\n\n# Define a function to plot for a given sleep stage DataFrame\ndef plot_sleep_stage(stage_df, stage_name, ax):\n    # Separate anomalies and non-anomalies\n    anomalies = stage_df[stage_df['anomalies'] == 1]\n    non_anomalies = stage_df[stage_df['anomalies'] == 0]\n    \n    # Plot non-anomalies\n    sns.lineplot(x=non_anomalies.index, y=non_anomalies[column_to_plot], ax=ax, label=f'{stage_name} Non-Anomaly', color='blue')\n    \n    # Plot anomalies\n    sns.lineplot(x=anomalies.index, y=anomalies[column_to_plot], ax=ax, label=f'{stage_name} Anomaly', color='red')\n    \n    # Title and labels\n    ax.set_title(f'{stage_name} Stage')\n    ax.set_xlabel('Index')\n    ax.set_ylabel(column_to_plot)\n    ax.legend()\n\n# Initialize a matplotlib figure with subplots\nfig, axes = plt.subplots(1, 1, figsize=(15, 10))\n# fig, axes = plt.subplots(3, 2, figsize=(15, 10))\n\n# Plot for each sleep stage on a different subplot\n# plot_sleep_stage(w_df, 'Sleep Stage W', axes[0, 0])\n# plot_sleep_stage(n1_df, 'Sleep Stage N1', axes[0, 1])\n# plot_sleep_stage(n2_df, 'Sleep Stage N2', axes[1, 0])\n# plot_sleep_stage(n3_df, 'Sleep Stage N3', axes[1, 1])\nplot_sleep_stage(scaled_r_df, 'Sleep Stage R', axes[2, 1])\n\n# Adjust layout for better spacing\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T05:39:09.736774Z","iopub.execute_input":"2024-10-13T05:39:09.737515Z","iopub.status.idle":"2024-10-13T05:39:10.061498Z","shell.execute_reply.started":"2024-10-13T05:39:09.737465Z","shell.execute_reply":"2024-10-13T05:39:10.060231Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Example column to plot\ncolumn_to_plot = 'Resp Abdominal'  # Replace with your actual column name\n\n# Define a function to plot for a given sleep stage DataFrame\ndef plot_sleep_stage(stage_df, stage_name, ax):\n    # Plot data, using hue to differentiate anomalies from non-anomalies\n    print(\"Hi\")\n    sns.lineplot(data=stage_df, x=stage_df.index, y=column_to_plot, hue='anomalies', \n                 palette={0: 'blue', 1: 'red'}, ax=ax, legend=False)\n\n    # Title and labels\n    ax.set_title(f'{stage_name} Stage')\n    ax.set_xlabel('Index')\n    ax.set_ylabel(column_to_plot)\n    print(\"Done\")\n\n# Initialize a matplotlib figure with subplots\nfig, axes = plt.subplots(3, 2, figsize=(15, 10))\n\n# Plot for each sleep stage on a different subplot\nplot_sleep_stage(w_df, 'Sleep Stage W', axes[0, 0])\nplot_sleep_stage(n1_df, 'Sleep Stage N1', axes[0, 1])\nplot_sleep_stage(n2_df, 'Sleep Stage N2', axes[1, 0])\nplot_sleep_stage(n3_df, 'Sleep Stage N3', axes[1, 1])\nplot_sleep_stage(r_df, 'Sleep Stage R', axes[2, 0])\n\n# Adjust layout for better spacing\nplt.tight_layout()\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-20T20:14:25.180638Z","iopub.execute_input":"2024-09-20T20:14:25.181Z","iopub.status.idle":"2024-09-20T21:03:30.943949Z","shell.execute_reply.started":"2024-09-20T20:14:25.180976Z","shell.execute_reply":"2024-09-20T21:03:30.943048Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Example column to plot\ncolumn_to_plot = 'Resp Rate'  # Replace with your actual column name\n\n# Define a function to plot for a given sleep stage DataFrame\ndef plot_sleep_stage(stage_df, stage_name, ax):\n    # Plot data, using hue to differentiate anomalies from non-anomalies\n    print(\"Hi\")\n    sns.lineplot(data=stage_df, x=stage_df.index, y=column_to_plot, hue='anomalies', \n                 palette={0: 'blue', 1: 'red'}, ax=ax, legend=False)\n\n    # Title and labels\n    ax.set_title(f'{stage_name} Stage')\n    ax.set_xlabel('Index')\n    ax.set_ylabel(column_to_plot)\n    print(\"Done\")\n\n# Initialize a matplotlib figure with subplots\nfig, axes = plt.subplots(3, 2, figsize=(15, 10))\n\n# Plot for each sleep stage on a different subplot\nplot_sleep_stage(w_df, 'Sleep Stage W', axes[0, 0])\nplot_sleep_stage(n1_df, 'Sleep Stage N1', axes[0, 1])\nplot_sleep_stage(n2_df, 'Sleep Stage N2', axes[1, 0])\nplot_sleep_stage(n3_df, 'Sleep Stage N3', axes[1, 1])\nplot_sleep_stage(r_df, 'Sleep Stage R', axes[2, 0])\n\n# Adjust layout for better spacing\nplt.tight_layout()\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-20T21:03:30.945136Z","iopub.execute_input":"2024-09-20T21:03:30.945485Z","iopub.status.idle":"2024-09-20T21:52:22.128282Z","shell.execute_reply.started":"2024-09-20T21:03:30.945454Z","shell.execute_reply":"2024-09-20T21:52:22.127436Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Statistics of the channel under consideration","metadata":{}},{"cell_type":"code","source":"# Example column to analyze\ncolumn_to_analyze = 'Resp Thoracic'  # Replace with your actual column name\n\n# Define a function to calculate and print statistics for a given sleep stage DataFrame\ndef print_statistics(stage_df, stage_name):\n    print(f\"\\nStatistics for {stage_name}:\")\n    \n    # Separate anomalies and non-anomalies\n    anomalies = stage_df[stage_df['anomalies'] == 1]\n    non_anomalies = stage_df[stage_df['anomalies'] == 0]\n\n    print(\"\\n--- Non-Anomalies ---\")\n    if not non_anomalies.empty:\n        # Print general statistics for non-anomalies\n        print(non_anomalies[column_to_analyze].describe())\n        # Calculate and print the mode for non-anomalies\n        print(f\"Mode: {non_anomalies[column_to_analyze].mode().values}\")\n\n    print(\"\\n--- Anomalies ---\")\n    if not anomalies.empty:\n        # Print general statistics for anomalies\n        print(anomalies[column_to_analyze].describe())\n        # Calculate and print the mode for anomalies\n        print(f\"Mode: {anomalies[column_to_analyze].mode().values}\")\n\n# Call the function for each sleep stage\nprint_statistics(w_df, 'Sleep Stage W')\nprint_statistics(n1_df, 'Sleep Stage N1')\nprint_statistics(n2_df, 'Sleep Stage N2')\nprint_statistics(n3_df, 'Sleep Stage N3')\nprint_statistics(r_df, 'Sleep Stage R')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-20T21:52:22.130653Z","iopub.execute_input":"2024-09-20T21:52:22.131047Z","iopub.status.idle":"2024-09-20T21:52:22.835589Z","shell.execute_reply.started":"2024-09-20T21:52:22.131018Z","shell.execute_reply":"2024-09-20T21:52:22.8346Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example column to analyze\ncolumn_to_analyze = 'Resp Abdominal'  # Replace with your actual column name\n\n# Define a function to calculate and print statistics for a given sleep stage DataFrame\ndef print_statistics(stage_df, stage_name):\n    print(f\"\\nStatistics for {stage_name}:\")\n    \n    # Separate anomalies and non-anomalies\n    anomalies = stage_df[stage_df['anomalies'] == 1]\n    non_anomalies = stage_df[stage_df['anomalies'] == 0]\n\n    print(\"\\n--- Non-Anomalies ---\")\n    if not non_anomalies.empty:\n        # Print general statistics for non-anomalies\n        print(non_anomalies[column_to_analyze].describe())\n        # Calculate and print the mode for non-anomalies\n        print(f\"Mode: {non_anomalies[column_to_analyze].mode().values}\")\n\n    print(\"\\n--- Anomalies ---\")\n    if not anomalies.empty:\n        # Print general statistics for anomalies\n        print(anomalies[column_to_analyze].describe())\n        # Calculate and print the mode for anomalies\n        print(f\"Mode: {anomalies[column_to_analyze].mode().values}\")\n\n# Call the function for each sleep stage\nprint_statistics(w_df, 'Sleep Stage W')\nprint_statistics(n1_df, 'Sleep Stage N1')\nprint_statistics(n2_df, 'Sleep Stage N2')\nprint_statistics(n3_df, 'Sleep Stage N3')\nprint_statistics(r_df, 'Sleep Stage R')","metadata":{"execution":{"iopub.status.busy":"2024-09-20T21:52:22.836743Z","iopub.execute_input":"2024-09-20T21:52:22.837048Z","iopub.status.idle":"2024-09-20T21:52:23.661335Z","shell.execute_reply.started":"2024-09-20T21:52:22.837024Z","shell.execute_reply":"2024-09-20T21:52:23.660483Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example column to analyze\ncolumn_to_analyze = 'Resp Rate'  # Replace with your actual column name\n\n# Define a function to calculate and print statistics for a given sleep stage DataFrame\ndef print_statistics(stage_df, stage_name):\n    print(f\"\\nStatistics for {stage_name}:\")\n    \n    # Separate anomalies and non-anomalies\n    anomalies = stage_df[stage_df['anomalies'] == 1]\n    non_anomalies = stage_df[stage_df['anomalies'] == 0]\n\n    print(\"\\n--- Non-Anomalies ---\")\n    if not non_anomalies.empty:\n        # Print general statistics for non-anomalies\n        print(non_anomalies[column_to_analyze].describe())\n        # Calculate and print the mode for non-anomalies\n        print(f\"Mode: {non_anomalies[column_to_analyze].mode().values}\")\n\n    print(\"\\n--- Anomalies ---\")\n    if not anomalies.empty:\n        # Print general statistics for anomalies\n        print(anomalies[column_to_analyze].describe())\n        # Calculate and print the mode for anomalies\n        print(f\"Mode: {anomalies[column_to_analyze].mode().values}\")\n\n# Call the function for each sleep stage\n# print_statistics(w_df, 'Sleep Stage W')\n# print_statistics(n1_df, 'Sleep Stage N1')\n# print_statistics(n2_df, 'Sleep Stage N2')\n# print_statistics(n3_df, 'Sleep Stage N3')\nprint_statistics(r_df, 'Sleep Stage R')","metadata":{"execution":{"iopub.status.busy":"2024-10-13T05:39:39.280922Z","iopub.execute_input":"2024-10-13T05:39:39.281823Z","iopub.status.idle":"2024-10-13T05:39:39.402578Z","shell.execute_reply.started":"2024-10-13T05:39:39.281789Z","shell.execute_reply":"2024-10-13T05:39:39.401577Z"},"trusted":true},"outputs":[],"execution_count":null}]}